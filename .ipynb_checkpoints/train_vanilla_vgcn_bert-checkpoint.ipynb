{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bcc0af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import argparse\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "# from tqdm import tqdm, trange\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from pytorch_pretrained_bert.modeling import BertModel, BertConfig, WEIGHTS_NAME, CONFIG_NAME\n",
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "from pytorch_pretrained_bert.optimization import BertAdam #, warmup_linear\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from utils import *\n",
    "\n",
    "import random\n",
    "random.seed(44)\n",
    "np.random.seed(44)\n",
    "torch.manual_seed(44)\n",
    "\n",
    "cuda_yes = torch.cuda.is_available()\n",
    "if cuda_yes:\n",
    "    torch.cuda.manual_seed_all(44)\n",
    "device = torch.device(\"cuda:0\" if cuda_yes else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d179e8f1",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1adc9310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vanilla_VGCN_BERT Start at: Tue Feb 22 10:55:10 2022\n",
      "\n",
      "----------------- Configure ----------------- \n",
      "  cfg_ds:  pheme \n",
      "  stop_words:  False \n",
      "  Learning_rate0:  1e-05 \t weight_decay:  0.01 \n",
      "  Loss_criterion:  cle \t softmax_before_mse  True \n",
      "  Dropout:  0.2 \t Run_adj:  pmi \t gcn_act_func: Relu \n",
      "  MAX_SEQ_LENGTH:  300 \n",
      "  perform_metrics_str:  ['weighted avg', 'f1-score'] \n",
      "  model_file_save:  Vanilla_VGCN_BERT1_model_pheme_cle_sw0.pt\n"
     ]
    }
   ],
   "source": [
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--ds', type=str, default='mr')\n",
    "# parser.add_argument('--load', type=int, default=0)\n",
    "# parser.add_argument('--sw', type=int, default='0')\n",
    "# parser.add_argument('--lr', type=float, default=1e-5)\n",
    "# parser.add_argument('--l2', type=float, default=0.01)\n",
    "# parser.add_argument('--model', type=str, default='Vanilla_VGCN_BERT')\n",
    "# args = parser.parse_args()\n",
    "cfg_ds = 'pheme'\n",
    "cfg_model_type = 'Vanilla_VGCN_BERT'\n",
    "cfg_stop_words = False\n",
    "will_train_model_from_checkpoint = False\n",
    "learning_rate0 = 1e-5\n",
    "l2_decay = 0.01\n",
    "\n",
    "total_train_epochs = 9 \n",
    "dropout_rate = 0.2  #0.5 # Dropout rate (1 - keep probability).\n",
    "\n",
    "if cfg_ds=='pheme':\n",
    "    batch_size = 16 #12   \n",
    "    learning_rate0 = 1e-5 #2e-5  \n",
    "    # l2_decay = 0.001\n",
    "    l2_decay = 0.01 #default\n",
    "    \n",
    "MAX_SEQ_LENGTH = 300\n",
    "gcn_embedding_dim=1\n",
    "gradient_accumulation_steps = 1\n",
    "bert_model_scale = 'bert-base-uncased'\n",
    "do_lower_case = True\n",
    "warmup_proportion = 0.1\n",
    "\n",
    "data_dir='data/dump_data'\n",
    "output_dir = 'output/'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "\n",
    "perform_metrics_str=['weighted avg','f1-score']\n",
    "\n",
    "# cfg_add_linear_mapping_term=False\n",
    "cfg_vocab_adj='pmi'\n",
    "# cfg_vocab_adj='all'\n",
    "# cfg_vocab_adj='tf'\n",
    "cfg_adj_npmi_threshold=0.2\n",
    "cfg_adj_tf_threshold=0\n",
    "classifier_act_func=nn.ReLU()\n",
    "\n",
    "resample_train_set=False # if mse and resample, then do resample\n",
    "do_softmax_before_mse=True\n",
    "cfg_loss_criterion='cle'\n",
    "model_file_save=cfg_model_type+str(gcn_embedding_dim)+'_model_'+cfg_ds+'_'+cfg_loss_criterion+'_'+\"sw\"+str(int(cfg_stop_words))+'.pt'\n",
    "\n",
    "print(cfg_model_type+' Start at:', time.asctime())\n",
    "print('\\n----------------- Configure -----------------',\n",
    "    '\\n  cfg_ds: ',cfg_ds,\n",
    "    '\\n  stop_words: ',cfg_stop_words,\n",
    "    '\\n  Learning_rate0: ',learning_rate0,'\\t weight_decay: ',l2_decay,\n",
    "    '\\n  Loss_criterion: ',cfg_loss_criterion,'\\t softmax_before_mse ',do_softmax_before_mse,\n",
    "    '\\n  Dropout: ',dropout_rate, '\\t Run_adj: ',cfg_vocab_adj, '\\t gcn_act_func: Relu',\n",
    "    '\\n  MAX_SEQ_LENGTH: ',MAX_SEQ_LENGTH,#'valid_data_taux',valid_data_taux,\n",
    "    '\\n  perform_metrics_str: ',perform_metrics_str,\n",
    "    '\\n  model_file_save: ',model_file_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c255444",
   "metadata": {},
   "source": [
    "### Prepare Dataset Load Vocabulary Adjacent Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8309f34a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- Prepare data set -----\n",
      "  Load/shuffle/seperate pheme dataset, and vocabulary graph adjacent matrix\n",
      "  Zero ratio(?>66%) for vocab adj 0th: 98.05823171\n"
     ]
    }
   ],
   "source": [
    "print('\\n----- Prepare data set -----')\n",
    "print('  Load/shuffle/seperate',cfg_ds,'dataset, and vocabulary graph adjacent matrix')\n",
    "\n",
    "objects=[]\n",
    "names = [ 'labels','train_y','train_y_prob', 'valid_y','valid_y_prob','test_y','test_y_prob', 'shuffled_clean_docs','vocab_adj_tf','vocab_adj_pmi','vocab_map'] \n",
    "for i in range(len(names)):\n",
    "    datafile=\"./\"+data_dir+\"/data_%s.%s\"%(cfg_ds,names[i])\n",
    "    with open(datafile, 'rb') as f:\n",
    "        objects.append(pkl.load(f, encoding='latin1'))\n",
    "lables_list,train_y, train_y_prob,valid_y,valid_y_prob,test_y,test_y_prob, shuffled_clean_docs,gcn_vocab_adj_tf,gcn_vocab_adj,gcn_vocab_map=tuple(objects)\n",
    "\n",
    "label2idx=lables_list[0]\n",
    "idx2label=lables_list[1]\n",
    "\n",
    "y=np.hstack((train_y,valid_y,test_y))\n",
    "y_prob=np.vstack((train_y_prob,valid_y_prob,test_y_prob))\n",
    "\n",
    "examples=[]\n",
    "for i,ts in enumerate(shuffled_clean_docs):\n",
    "    ex=InputExample(i, ts.strip(), confidence=y_prob[i],label=y[i])\n",
    "    examples.append(ex)\n",
    "\n",
    "num_classes=len(label2idx)\n",
    "gcn_vocab_size=len(gcn_vocab_map)\n",
    "train_size = len(train_y)\n",
    "valid_size = len(valid_y)\n",
    "test_size = len(test_y)\n",
    "\n",
    "indexs = np.arange(0, len(examples))\n",
    "train_examples = [examples[i] for i in indexs[:train_size]]\n",
    "valid_examples = [examples[i] for i in indexs[train_size:train_size+valid_size]]\n",
    "test_examples = [examples[i] for i in indexs[train_size+valid_size:train_size+valid_size+test_size]]\n",
    "\n",
    "if cfg_adj_tf_threshold>0:\n",
    "    gcn_vocab_adj_tf.data *= (gcn_vocab_adj_tf.data > cfg_adj_tf_threshold)\n",
    "    gcn_vocab_adj_tf.eliminate_zeros()\n",
    "if cfg_adj_npmi_threshold>0:\n",
    "    gcn_vocab_adj.data *= (gcn_vocab_adj.data > cfg_adj_npmi_threshold)\n",
    "    gcn_vocab_adj.eliminate_zeros()\n",
    "\n",
    "if cfg_vocab_adj=='pmi':\n",
    "    gcn_vocab_adj_list=[gcn_vocab_adj]\n",
    "elif cfg_vocab_adj=='tf':\n",
    "    gcn_vocab_adj_list=[gcn_vocab_adj_tf]\n",
    "elif cfg_vocab_adj=='all':\n",
    "    gcn_vocab_adj_list=[gcn_vocab_adj_tf,gcn_vocab_adj]\n",
    "\n",
    "norm_gcn_vocab_adj_list=[]\n",
    "for i in range(len(gcn_vocab_adj_list)):\n",
    "    adj=gcn_vocab_adj_list[i] #.tocsr() #(lr是用非norm时的1/10)\n",
    "    print('  Zero ratio(?>66%%) for vocab adj %dth: %.8f'%(i, 100*(1-adj.count_nonzero()/(adj.shape[0]*adj.shape[1]))))\n",
    "    adj=normalize_adj(adj)\n",
    "    norm_gcn_vocab_adj_list.append(sparse_scipy2torch(adj.tocoo()).to(device))\n",
    "gcn_adj_list=norm_gcn_vocab_adj_list\n",
    "\n",
    "\n",
    "del gcn_vocab_adj_tf,gcn_vocab_adj,gcn_vocab_adj_list\n",
    "gc.collect()\n",
    "\n",
    "train_classes_num, train_classes_weight = get_class_count_and_weight(train_y,len(label2idx))\n",
    "loss_weight=torch.tensor(train_classes_weight).to(device)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model_scale, do_lower_case=do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1a7bb88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train_classes count: [3620, 1558]\n",
      "  Num examples for train = 5178 , after weight sample: 5184\n",
      "  Num examples for validate = 1672\n",
      "  Batch size = 16\n",
      "  Num steps = 2916\n"
     ]
    }
   ],
   "source": [
    "def get_pytorch_dataloader(examples, tokenizer, batch_size, shuffle_choice, classes_weight=None, total_resample_size=-1):\n",
    "    ds = CorpusDataset(examples, tokenizer, gcn_vocab_map, MAX_SEQ_LENGTH, gcn_embedding_dim)\n",
    "    if shuffle_choice==0: # shuffle==False\n",
    "        return DataLoader(dataset=ds,\n",
    "                                batch_size=batch_size,\n",
    "                                shuffle=False,\n",
    "                                num_workers=4,\n",
    "                                collate_fn=ds.pad)\n",
    "    elif shuffle_choice==1: # shuffle==True\n",
    "        return DataLoader(dataset=ds,\n",
    "                                batch_size=batch_size,\n",
    "                                shuffle=True,\n",
    "                                num_workers=4,\n",
    "                                collate_fn=ds.pad)\n",
    "    elif shuffle_choice==2: #weighted resampled\n",
    "        assert classes_weight is not None\n",
    "        assert total_resample_size>0\n",
    "        weights = [classes_weight[0] if label == 0 else classes_weight[1] if label == 1 else classes_weight[2] for _,_,_,_,label in dataset]\n",
    "        sampler = WeightedRandomSampler(weights, num_samples=total_resample_size, replacement=True)\n",
    "        return DataLoader(dataset=ds,\n",
    "                                batch_size=batch_size,\n",
    "                                sampler=sampler,\n",
    "                                num_workers=4,\n",
    "                                collate_fn=ds.pad)\n",
    "\n",
    "train_dataloader = get_pytorch_dataloader(train_examples, tokenizer, batch_size, shuffle_choice=0 )\n",
    "valid_dataloader = get_pytorch_dataloader(valid_examples, tokenizer, batch_size, shuffle_choice=0 )\n",
    "test_dataloader = get_pytorch_dataloader(test_examples, tokenizer, batch_size, shuffle_choice=0 )\n",
    "\n",
    "\n",
    "# total_train_steps = int(len(train_examples) / batch_size / gradient_accumulation_steps * total_train_epochs)\n",
    "total_train_steps = int(len(train_dataloader) / gradient_accumulation_steps * total_train_epochs)\n",
    "\n",
    "print('  Train_classes count:', train_classes_num)\n",
    "print('  Num examples for train =',len(train_examples),', after weight sample:',len(train_dataloader)*batch_size)\n",
    "print(\"  Num examples for validate = %d\"% len(valid_examples))\n",
    "print(\"  Batch size = %d\"% batch_size)\n",
    "print(\"  Num steps = %d\"% total_train_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba20fde2",
   "metadata": {},
   "source": [
    "### Train vanilla_vgcn_bert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "408d912e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, gcn_adj_list,predict_dataloader, batch_size, epoch_th, dataset_name):\n",
    "    # print(\"***** Running prediction *****\")\n",
    "    model.eval()\n",
    "    predict_out = []\n",
    "    all_label_ids = []\n",
    "    ev_loss=0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        for batch in predict_dataloader:\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            input_ids, input_mask, segment_ids, y_prob, label_ids, gcn_swop_eye = batch\n",
    "            # the parameter label_ids is None, model return the prediction score\n",
    "            logits = model(gcn_adj_list,gcn_swop_eye,input_ids,  segment_ids, input_mask)\n",
    "\n",
    "            if cfg_loss_criterion=='mse':\n",
    "                if do_softmax_before_mse:\n",
    "                    logits=F.softmax(logits,-1)\n",
    "                loss = F.mse_loss(logits, y_prob)\n",
    "            else:\n",
    "                if loss_weight is None:\n",
    "                    loss = F.cross_entropy(logits.view(-1, num_classes), label_ids)\n",
    "                else:\n",
    "                    loss = F.cross_entropy(logits.view(-1, num_classes), label_ids)\n",
    "            ev_loss+=loss.item()\n",
    "\n",
    "            _, predicted = torch.max(logits, -1)\n",
    "            predict_out.extend(predicted.tolist())\n",
    "            all_label_ids.extend(label_ids.tolist())\n",
    "            eval_accuracy=predicted.eq(label_ids).sum().item()\n",
    "            total += len(label_ids)\n",
    "            correct += eval_accuracy\n",
    "\n",
    "        f1_metrics=f1_score(np.array(all_label_ids).reshape(-1),\n",
    "            np.array(predict_out).reshape(-1), average='weighted')\n",
    "        print(\"Report:\\n\"+classification_report(np.array(all_label_ids).reshape(-1),\n",
    "            np.array(predict_out).reshape(-1),digits=4))\n",
    "\n",
    "    ev_acc = correct/total\n",
    "    end = time.time()\n",
    "    print('Epoch : %d, %s: %.3f Acc : %.3f on %s, Spend:%.3f minutes for evaluation'\n",
    "        % (epoch_th, ' '.join(perform_metrics_str), 100*f1_metrics, 100.*ev_acc, dataset_name,(end-start)/60.0))\n",
    "    print('--------------------------------------------------------------')\n",
    "    return ev_loss, ev_acc, f1_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc1d06d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- Running training -----\n"
     ]
    }
   ],
   "source": [
    "from  model_vanilla_vgcn_bert import Vanilla_VGCN_Bert\n",
    "print(\"\\n----- Running training -----\")\n",
    "if will_train_model_from_checkpoint and os.path.exists(os.path.join(output_dir, model_file_save)):\n",
    "    checkpoint = torch.load(os.path.join(output_dir, model_file_save), map_location='cpu')\n",
    "    if 'step' in checkpoint:\n",
    "        prev_save_step=checkpoint['step']\n",
    "        start_epoch = checkpoint['epoch']\n",
    "    else:\n",
    "        prev_save_step=-1\n",
    "        start_epoch = checkpoint['epoch']+1\n",
    "    start_epoch = checkpoint['epoch']+1\n",
    "    valid_acc_prev = checkpoint['valid_acc']\n",
    "    perform_metrics_prev = checkpoint['perform_metrics']\n",
    "    model = Vanilla_VGCN_Bert.from_pretrained(bert_model_scale, state_dict=checkpoint['model_state'], gcn_adj_dim=gcn_vocab_size, gcn_adj_num=len(gcn_adj_list),gcn_embedding_dim=gcn_embedding_dim, num_labels=len(label2idx))\n",
    "    print('Loaded the pretrain model:',model_file_save,', epoch:',checkpoint['epoch'],'valid acc:',\n",
    "        checkpoint['valid_acc'],' '.join(perform_metrics_str)+'_valid:', checkpoint['perform_metrics'])\n",
    "\n",
    "else:\n",
    "    start_epoch = 0\n",
    "    valid_acc_prev = 0\n",
    "    perform_metrics_prev = 0\n",
    "    model = Vanilla_VGCN_Bert.from_pretrained(bert_model_scale, gcn_adj_dim=gcn_vocab_size, gcn_adj_num=len(gcn_adj_list),gcn_embedding_dim=gcn_embedding_dim, num_labels=len(label2idx))\n",
    "    prev_save_step=-1\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "optimizer = BertAdam(model.parameters(), lr=learning_rate0, warmup=warmup_proportion, t_total=total_train_steps, weight_decay=l2_decay)\n",
    "\n",
    "train_start = time.time()\n",
    "global_step_th = int(len(train_examples) / batch_size / gradient_accumulation_steps * start_epoch)\n",
    "\n",
    "all_loss_list={'train':[],'valid':[],'test':[]}\n",
    "all_f1_list={'train':[],'valid':[],'test':[]}\n",
    "for epoch in range(start_epoch, total_train_epochs):\n",
    "    tr_loss = 0\n",
    "    ep_train_start = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    # for step, batch in enumerate(tqdm(train_dataloader, desc=\"Iteration\")):\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        if prev_save_step >-1:\n",
    "            if step<=prev_save_step: continue\n",
    "        if prev_save_step >-1: \n",
    "            prev_save_step=-1\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        input_ids, input_mask, segment_ids, y_prob, label_ids, gcn_swop_eye = batch\n",
    "        \n",
    "        logits = model(gcn_adj_list, gcn_swop_eye, input_ids, segment_ids, input_mask)\n",
    "\n",
    "        if cfg_loss_criterion=='mse':\n",
    "            if do_softmax_before_mse:\n",
    "                logits=F.softmax(logits,-1)\n",
    "            loss = F.mse_loss(logits, y_prob)\n",
    "        else:\n",
    "            if loss_weight is None:\n",
    "                loss = F.cross_entropy(logits, label_ids)\n",
    "            else:\n",
    "                loss = F.cross_entropy(logits.view(-1, num_classes), label_ids, loss_weight.float())\n",
    "\n",
    "        if gradient_accumulation_steps > 1:\n",
    "            loss = loss / gradient_accumulation_steps\n",
    "        loss.backward()\n",
    "\n",
    "        tr_loss += loss.item()\n",
    "        if (step + 1) % gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            global_step_th += 1\n",
    "        if step % 40 == 0:\n",
    "            print(\"Epoch:{}-{}/{}, Train {} Loss: {}, Cumulated time: {}m \".format(epoch, step, len(train_dataloader), cfg_loss_criterion,loss.item(),(time.time() - train_start)/60.0))\n",
    "\n",
    "    print('--------------------------------------------------------------')\n",
    "    valid_loss,valid_acc,perform_metrics = evaluate(model, gcn_adj_list, valid_dataloader, batch_size, epoch, 'Valid_set')\n",
    "    test_loss,_,test_f1 = evaluate(model, gcn_adj_list, test_dataloader, batch_size, epoch, 'Test_set')\n",
    "    all_loss_list['train'].append(tr_loss)\n",
    "    all_loss_list['valid'].append(valid_loss)\n",
    "    all_loss_list['test'].append(test_loss)\n",
    "    all_f1_list['valid'].append(perform_metrics)\n",
    "    all_f1_list['test'].append(test_f1)\n",
    "    print(\"Epoch:{} completed, Total Train Loss:{}, Valid Loss:{}, Spend {}m \".format(epoch, tr_loss, valid_loss, (time.time() - train_start)/60.0))\n",
    "    # Save a checkpoint\n",
    "    # if valid_acc > valid_acc_prev:\n",
    "    if perform_metrics > perform_metrics_prev:\n",
    "        to_save={'epoch': epoch, 'model_state': model.state_dict(),\n",
    "                    'valid_acc': valid_acc, 'lower_case': do_lower_case,\n",
    "                    'perform_metrics':perform_metrics}\n",
    "        torch.save(to_save, os.path.join(output_dir, model_file_save))\n",
    "        # valid_acc_prev = valid_acc\n",
    "        perform_metrics_prev = perform_metrics\n",
    "        test_f1_when_valid_best=test_f1\n",
    "        # train_f1_when_valid_best=tr_f1\n",
    "        valid_f1_best_epoch=epoch\n",
    "\n",
    "print('\\n**Optimization Finished!,Total spend:',(time.time() - train_start)/60.0)\n",
    "print(\"**Valid weighted F1: %.3f at %d epoch.\"%(100*perform_metrics_prev,valid_f1_best_epoch))\n",
    "print(\"**Test weighted F1 when valid best: %.3f\"%(100*test_f1_when_valid_best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df44fac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
