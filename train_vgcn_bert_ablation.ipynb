{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f0a974b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from pytorch_pretrained_bert.modeling import BertModel, BertConfig, WEIGHTS_NAME, CONFIG_NAME\n",
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "from pytorch_pretrained_bert.optimization import BertAdam  # , warmup_linear\n",
    "\n",
    "\n",
    "from utils import *\n",
    "from model_vgcn_bert_ablation import VGCN_Bert_Ablation\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b426dc",
   "metadata": {},
   "source": [
    "#### Step 1:   Configurations for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b80f6e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------STEP 1: CONFIGURATIONS FOR TRAINING--------\n",
      "Dataset:  weibo\n",
      "Will Load Model from Checkpoint:  False\n",
      "Will Delete Stop Words:  True\n",
      "Vocab GCN Hidden Dim: vocab_size -> 128 -> 1\n",
      "Learning Rate0:  1e-05\n",
      "Weight Decay:  0.01\n",
      "Loss Criterion:  cross_entropy\n",
      "Will Perform Softmax before MSE:  True\n",
      "Vocab Adjcent:  all\n",
      "MAX_SEQ_LENGTH:  250\n",
      "Perform Metrics:  ['weighted avg', 'f1-score']\n",
      "Saved Model File Name:  Model_VGCN_BERT_Ablation_weibo_text_comments.pt\n"
     ]
    }
   ],
   "source": [
    "# parser = argparse.ArgumentParser()\n",
    "# # ds = dataset, sw = stopwords, lr = learning rate, l2 = L2 regularization\n",
    "# parser.add_argument('--ds', type=str, default='pheme')\n",
    "# parser.add_argument('--load', type=int, default='0')\n",
    "# parser.add_argument('--sw', type=int, default='1')\n",
    "# parser.add_argument('--dim', type=int, default='16')\n",
    "# parser.add_argument('--lr', type=float, default=1e-5)  # 2e-5\n",
    "# parser.add_argument('--l2', type=float, default=0.01)  # 0.001\n",
    "# parser.add_argument('--model', type=str, default='VGCN_BERT')\n",
    "# args = parser.parse_args()\n",
    "args = {\"ds\": \"weibo\", \"load\": 0, \"sw\": 1, \"dim\": 1,\n",
    "        \"lr\": 1e-5, \"l2\": 0.01, \"model\": \"VGCN_BERT_Ablation\"}\n",
    "\n",
    "# config_dataset = args.ds\n",
    "# config_load_model_from_checkpoint = True if args.load == 1 else False\n",
    "# config_use_stopwords = True if args.sw == 1 else False\n",
    "# config_gcn_embedding_dim = args.dim\n",
    "# config_learning_rate0 = args.lr\n",
    "# config_l2_decay = args.l2\n",
    "# config_model_type = args.model\n",
    "\n",
    "config_dataset = args[\"ds\"]\n",
    "config_load_model_from_checkpoint = True if args[\"load\"] == 1 else False\n",
    "config_use_stopwords = True if args[\"sw\"] == 1 else False\n",
    "config_gcn_embedding_dim = args[\"dim\"]\n",
    "config_learning_rate0 = args[\"lr\"]\n",
    "config_l2_decay = args[\"l2\"]\n",
    "config_model_type = args[\"model\"]\n",
    "config_data_type = \"text_comments\" # text_comments / text_only / comments_only\n",
    "\n",
    "\n",
    "config_warmup_proportion = 0.1\n",
    "config_vocab_adj = 'all'  # pmi / tf / all\n",
    "config_adj_npmi_threshold = 0.2\n",
    "config_adj_tf_threshold = 0\n",
    "config_loss_criterion = 'cross_entropy'\n",
    "\n",
    "MAX_SEQ_LENGTH = 250\n",
    "total_train_epochs = 9\n",
    "batch_size = 16  # 12\n",
    "gradient_accumulation_steps = 1\n",
    "\n",
    "do_lower_case = True\n",
    "perform_metrics_str = ['weighted avg', 'f1-score']\n",
    "do_softmax_before_mse = True\n",
    "\n",
    "if config_dataset == 'pheme':\n",
    "    bert_model_scale = 'bert-base-uncased'\n",
    "elif config_dataset == 'weibo':\n",
    "    bert_model_scale = 'bert-base-chinese'\n",
    "\n",
    "data_dir = './prepared_data/' + config_dataset + '_' + config_data_type\n",
    "output_dir = './model_output/ablation_tests/'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "\n",
    "\n",
    "model_save_file = 'Model_' + config_model_type + '_' \\\n",
    "                + config_dataset + '_' + config_data_type + '.pt'\n",
    "\n",
    "print('----------STEP 1: CONFIGURATIONS FOR TRAINING--------')\n",
    "print('Dataset: ', config_dataset)\n",
    "print('Will Load Model from Checkpoint: ', config_load_model_from_checkpoint)\n",
    "print('Will Delete Stop Words: ', config_use_stopwords)\n",
    "print('Vocab GCN Hidden Dim: vocab_size -> 128 -> ' + str(config_gcn_embedding_dim))\n",
    "print('Learning Rate0: ', config_learning_rate0)\n",
    "print('Weight Decay: ', config_l2_decay)\n",
    "print('Loss Criterion: ', config_loss_criterion)\n",
    "print('Will Perform Softmax before MSE: ', do_softmax_before_mse)\n",
    "print('Vocab Adjcent: ', config_vocab_adj)\n",
    "print('MAX_SEQ_LENGTH: ', MAX_SEQ_LENGTH)\n",
    "print('Perform Metrics: ', perform_metrics_str)\n",
    "print('Saved Model File Name: ', model_save_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca91c24",
   "metadata": {},
   "source": [
    "#### Step 2.1: Prepare Dataset & Load Vocabulary Adjacent Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d69faa5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------STEP 2: PREPARE DATASET & LOAD VOCABULARY ADJACENT MATRIX----------\n",
      " Load and seperate weibo dataset, with vocabulary graph adjacent matrix\n"
     ]
    }
   ],
   "source": [
    "print('----------STEP 2: PREPARE DATASET & LOAD VOCABULARY ADJACENT MATRIX----------')\n",
    "print(' Load and seperate', config_dataset, 'dataset, with vocabulary graph adjacent matrix')\n",
    "\n",
    "objects = []\n",
    "names = ['index_label', 'train_label', 'train_label_prob', 'test_label',\n",
    "         'test_label_prob', 'clean_docs', 'vocab_adj_tf', 'vocab_adj_pmi', 'vocab_map']\n",
    "\n",
    "for i in range(len(names)):\n",
    "    datafile = data_dir + \"/data_%s.%s\" % (config_dataset, names[i])\n",
    "    with open(datafile, 'rb') as f:\n",
    "        objects.append(pkl.load(f, encoding='latin1'))\n",
    "\n",
    "index_labels_list, train_label, train_label_prob, test_label, test_label_prob, shuffled_clean_docs, gcn_vocab_adj_tf, gcn_vocab_adj_pmi, gcn_vocab_map = tuple(objects)\n",
    "\n",
    "label2idx = index_labels_list[0]\n",
    "idx2label = index_labels_list[1]\n",
    "\n",
    "all_labels = np.hstack((train_label, test_label))\n",
    "all_labels_prob = np.vstack((train_label_prob, test_label_prob))\n",
    "\n",
    "examples = []\n",
    "for i, text in enumerate(shuffled_clean_docs):\n",
    "    example = InputExample(i, text.strip(), confidence=all_labels_prob[i], label=all_labels[i])\n",
    "    examples.append(example)\n",
    "\n",
    "num_classes = len(label2idx)\n",
    "gcn_vocab_size = len(gcn_vocab_map)\n",
    "train_size = len(train_label)\n",
    "test_size = len(test_label)\n",
    "\n",
    "indexs = np.arange(0, len(examples))\n",
    "train_examples = [examples[i] for i in indexs[:train_size]]\n",
    "test_examples = [examples[i] for i in indexs[train_size:train_size + test_size]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2c84af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero ratio for vocab adj 0th: 50.73316231\n",
      "Zero ratio for vocab adj 1th: 95.78563558\n"
     ]
    }
   ],
   "source": [
    "if config_adj_tf_threshold > 0:\n",
    "    gcn_vocab_adj_tf.data *= (gcn_vocab_adj_tf.data > config_adj_tf_threshold)\n",
    "    gcn_vocab_adj_tf.eliminate_zeros()\n",
    "if config_adj_npmi_threshold > 0:\n",
    "    gcn_vocab_adj_pmi.data *= (gcn_vocab_adj_pmi.data > config_adj_npmi_threshold)\n",
    "    gcn_vocab_adj_pmi.eliminate_zeros()\n",
    "\n",
    "if config_vocab_adj == 'pmi':\n",
    "    gcn_vocab_adj_list = [gcn_vocab_adj_pmi]\n",
    "elif config_vocab_adj == 'tf':\n",
    "    gcn_vocab_adj_list = [gcn_vocab_adj_tf]\n",
    "elif config_vocab_adj == 'all':\n",
    "    gcn_vocab_adj_list = [gcn_vocab_adj_tf, gcn_vocab_adj_pmi]\n",
    "\n",
    "norm_gcn_vocab_adj_list = []\n",
    "for i in range(len(gcn_vocab_adj_list)):\n",
    "    adj = gcn_vocab_adj_list[i]\n",
    "\n",
    "    print('Zero ratio for vocab adj %dth: %.8f' %\n",
    "          (i, 100 * (1 - adj.count_nonzero() / (adj.shape[0] * adj.shape[1]))))\n",
    "\n",
    "    adj = normalize_adj(adj)\n",
    "    norm_gcn_vocab_adj_list.append(sparse_scipy2torch(adj.tocoo()).to(device))\n",
    "\n",
    "gcn_adj_list = norm_gcn_vocab_adj_list\n",
    "\n",
    "\n",
    "train_classes_num, train_classes_weight = get_class_count_and_weight(train_label, len(label2idx))\n",
    "loss_weight = torch.tensor(train_classes_weight).to(device)\n",
    "loss_weight = torch.tensor(loss_weight, dtype=torch.float32).to(device)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model_scale, do_lower_case=do_lower_case)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7701b8",
   "metadata": {},
   "source": [
    "#### Step 2.2:   Prepare PyTorch DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "174db10b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Classes Count:  [2108, 2089]\n",
      "Batch size:  16\n",
      "Num steps:  2367\n",
      "Number of Examples for Training:  4197\n",
      "Number of Examples for Training After Dataloader:  4208\n",
      "Number of Examples for Test:  1727\n"
     ]
    }
   ],
   "source": [
    "def get_pytorch_dataloader(examples, tokenizer, batch_size):\n",
    "    dataset = CorpusDataset(examples, tokenizer, gcn_vocab_map, MAX_SEQ_LENGTH, config_gcn_embedding_dim)\n",
    "    return DataLoader(dataset=dataset, batch_size=batch_size, shuffle=False, num_workers=4, collate_fn=dataset.pad)\n",
    "\n",
    "\n",
    "train_dataloader = get_pytorch_dataloader(train_examples, tokenizer, batch_size)\n",
    "test_dataloader = get_pytorch_dataloader(test_examples, tokenizer, batch_size)\n",
    "\n",
    "total_train_steps = int(len(train_dataloader) / gradient_accumulation_steps * total_train_epochs)\n",
    "\n",
    "print('Train Classes Count: ', train_classes_num)\n",
    "print('Batch size: ', batch_size)\n",
    "print('Num steps: ', total_train_steps)\n",
    "print('Number of Examples for Training: ', len(train_examples))\n",
    "print('Number of Examples for Training After Dataloader: ', len(train_dataloader) * batch_size)\n",
    "print('Number of Examples for Test: ', len(test_examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f295bf3",
   "metadata": {},
   "source": [
    "#### Step 3.1:  Define Evaluating Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5fd9d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, examples, tokenizer, batch_size):\n",
    "    dataloader = get_pytorch_dataloader(examples, tokenizer, batch_size)\n",
    "    predict_out = []\n",
    "    confidence_out = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            input_ids, input_mask, segment_ids, _, _, gcn_swop_eye = batch\n",
    "\n",
    "            _, _, _, score_out = model(gcn_adj_list, gcn_swop_eye,\n",
    "                                 input_ids, segment_ids, input_mask)\n",
    "            if config_loss_criterion == 'mse' and do_softmax_before_mse:\n",
    "                score_out = torch.nn.functional.softmax(score_out, dim=-1)\n",
    "            predict_out.extend(score_out.max(1)[1].tolist())\n",
    "            confidence_out.extend(score_out.max(1)[0].tolist())\n",
    "\n",
    "    return np.array(predict_out).reshape(-1), np.array(confidence_out).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c0dd260",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, gcn_adj_list, predict_dataloader, epoch_th, dataset_name):\n",
    "    model.eval()\n",
    "    predict_out = []\n",
    "    all_label_ids = []\n",
    "    ev_loss = 0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        for batch in predict_dataloader:\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            input_ids, input_mask, segment_ids, y_prob, label_ids, gcn_swop_eye = batch\n",
    "            _, _, _, logits = model(gcn_adj_list, gcn_swop_eye,input_ids,  segment_ids, input_mask)\n",
    "\n",
    "            if config_loss_criterion == 'mse':\n",
    "                if do_softmax_before_mse:\n",
    "                    logits = F.softmax(logits, -1)\n",
    "                loss = F.mse_loss(logits, y_prob)\n",
    "            else:\n",
    "                loss = F.cross_entropy(logits.view(-1, num_classes), label_ids)\n",
    "            ev_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(logits, -1)\n",
    "            predict_out.extend(predicted.tolist())\n",
    "            all_label_ids.extend(label_ids.tolist())\n",
    "            eval_accuracy = predicted.eq(label_ids).sum().item()\n",
    "            total += len(label_ids)\n",
    "            correct += eval_accuracy\n",
    "\n",
    "        f1_metrics = f1_score(np.array(all_label_ids).reshape(-1), np.array(predict_out).reshape(-1), average='weighted')\n",
    "        print(\"Report:\\n\" + classification_report(np.array(all_label_ids).reshape(-1), np.array(predict_out).reshape(-1), digits=4))\n",
    "\n",
    "    ev_acc = correct/total\n",
    "    end = time.time()\n",
    "    print('Epoch : %d, %s: %.3f Acc : %.3f on %s, Spend:%.3f minutes for evaluation'\n",
    "          % (epoch_th, ' '.join(perform_metrics_str), 100 * f1_metrics, 100. * ev_acc, dataset_name, (end - start) / 60.0))\n",
    "    print('*' * 50)\n",
    "    return ev_loss, ev_acc, f1_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6501712a",
   "metadata": {},
   "source": [
    "#### Step 3.2:   Load / Initialize VGCN_BERT_Ablation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32f499d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------STEP 3: START TRAINING VGCN_BERT_ABLATION MODEL----------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VGCN_Bert_Ablation(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=1536, out_features=2, bias=True)\n",
       "  (vocab_gcn): VocabGraphConvolution(\n",
       "    (fc_hc): Linear(in_features=128, out_features=1, bias=True)\n",
       "    (act_func): ReLU()\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('----------STEP 3: START TRAINING VGCN_BERT_ABLATION MODEL----------')\n",
    "\n",
    "if config_load_model_from_checkpoint and os.path.exists(os.path.join(output_dir, model_save_file)):\n",
    "    checkpoint = torch.load(os.path.join(output_dir, model_save_file), map_location='cpu')\n",
    "    if 'step' in checkpoint:\n",
    "        prev_save_step = checkpoint['step']\n",
    "        start_epoch = checkpoint['epoch']\n",
    "    else:\n",
    "        prev_save_step = -1\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "\n",
    "    test_acc_prev = checkpoint['test_acc']\n",
    "    perform_metrics_prev = checkpoint['perform_metrics']\n",
    "    model = VGCN_Bert.from_pretrained(bert_model_scale, state_dict=checkpoint['model_state'], gcn_adj_dim=gcn_vocab_size, \n",
    "        gcn_adj_num=len(gcn_adj_list), gcn_embedding_dim=config_gcn_embedding_dim, num_labels=len(label2idx))\n",
    "\n",
    "    pretrained_dict = checkpoint['model_state']\n",
    "    net_state_dict = model.state_dict()\n",
    "    pretrained_dict_selected = {\n",
    "        k: v for k, v in pretrained_dict.items() if k in net_state_dict}\n",
    "    net_state_dict.update(pretrained_dict_selected)\n",
    "    model.load_state_dict(net_state_dict)\n",
    "\n",
    "    print('Loaded the pre-trained model:', model_save_file, ', epoch:', checkpoint['epoch'], 'step:', prev_save_step, 'test acc:',\n",
    "          checkpoint['test_acc'], ' '.join(perform_metrics_str) + '_test:', checkpoint['perform_metrics'])\n",
    "\n",
    "else:\n",
    "    start_epoch = 0\n",
    "    test_acc_prev = 0\n",
    "    perform_metrics_prev = 0\n",
    "    model = VGCN_Bert_Ablation.from_pretrained(bert_model_scale, gcn_adj_dim=gcn_vocab_size, gcn_adj_num=len(\n",
    "        gcn_adj_list), gcn_embedding_dim=config_gcn_embedding_dim, num_labels=len(label2idx))\n",
    "    prev_save_step = -1\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b09fa76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0-0/263, Train cross_entropy Loss: 0.7008556127548218, Cumulated Time: 0.0327737291653951m \n",
      "Epoch:0-40/263, Train cross_entropy Loss: 0.6344537138938904, Cumulated Time: 2.5530888120333355m \n",
      "Epoch:0-80/263, Train cross_entropy Loss: 0.2743552029132843, Cumulated Time: 6.19329073826472m \n",
      "Epoch:0-120/263, Train cross_entropy Loss: 0.301933616399765, Cumulated Time: 10.385427288214366m \n",
      "Epoch:0-160/263, Train cross_entropy Loss: 0.18648116290569305, Cumulated Time: 14.867073508103688m \n",
      "Epoch:0-200/263, Train cross_entropy Loss: 0.05361226946115494, Cumulated Time: 19.36471599737803m \n",
      "Epoch:0-240/263, Train cross_entropy Loss: 0.3833027482032776, Cumulated Time: 23.91282465457916m \n",
      "**************************************************\n",
      "Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9451    0.9483    0.9467       889\n",
      "           1     0.9449    0.9415    0.9432       838\n",
      "\n",
      "    accuracy                         0.9450      1727\n",
      "   macro avg     0.9450    0.9449    0.9449      1727\n",
      "weighted avg     0.9450    0.9450    0.9450      1727\n",
      "\n",
      "Epoch : 0, weighted avg f1-score: 94.499 Acc : 94.499 on Test_set, Spend:4.235 minutes for evaluation\n",
      "**************************************************\n",
      "Epoch:0 Completed, Total Train Loss:89.38339090906084, Test Loss:17.67162064090371, Spend 30.59770446618398m \n",
      "Epoch:1-0/263, Train cross_entropy Loss: 0.18401873111724854, Cumulated Time: 30.739402310053507m \n",
      "Epoch:1-40/263, Train cross_entropy Loss: 0.48788154125213623, Cumulated Time: 35.32562826474508m \n",
      "Epoch:1-80/263, Train cross_entropy Loss: 0.04061463475227356, Cumulated Time: 39.93426009813945m \n",
      "Epoch:1-120/263, Train cross_entropy Loss: 0.01195282768458128, Cumulated Time: 44.53637079000473m \n",
      "Epoch:1-160/263, Train cross_entropy Loss: 0.023077907040715218, Cumulated Time: 49.12616473038991m \n",
      "Epoch:1-200/263, Train cross_entropy Loss: 0.021507492288947105, Cumulated Time: 53.65724613666534m \n",
      "Epoch:1-240/263, Train cross_entropy Loss: 0.17444130778312683, Cumulated Time: 58.167453130086265m \n",
      "**************************************************\n",
      "Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9796    0.9741    0.9769       889\n",
      "           1     0.9727    0.9785    0.9756       838\n",
      "\n",
      "    accuracy                         0.9763      1727\n",
      "   macro avg     0.9762    0.9763    0.9762      1727\n",
      "weighted avg     0.9763    0.9763    0.9763      1727\n",
      "\n",
      "Epoch : 1, weighted avg f1-score: 97.626 Acc : 97.626 on Test_set, Spend:4.234 minutes for evaluation\n",
      "**************************************************\n",
      "Epoch:1 Completed, Total Train Loss:34.19067840324715, Test Loss:7.941110951593146, Spend 64.84133387804032m \n",
      "Epoch:2-0/263, Train cross_entropy Loss: 0.011532546952366829, Cumulated Time: 65.00910068353018m \n",
      "Epoch:2-40/263, Train cross_entropy Loss: 0.15924295783042908, Cumulated Time: 69.55787251790365m \n",
      "Epoch:2-80/263, Train cross_entropy Loss: 0.004916396923363209, Cumulated Time: 74.15345330238343m \n",
      "Epoch:2-120/263, Train cross_entropy Loss: 0.005580048076808453, Cumulated Time: 78.75043470859528m \n",
      "Epoch:2-160/263, Train cross_entropy Loss: 0.007258288562297821, Cumulated Time: 83.3604660153389m \n",
      "Epoch:2-200/263, Train cross_entropy Loss: 0.07150925695896149, Cumulated Time: 87.93519619305928m \n",
      "Epoch:2-240/263, Train cross_entropy Loss: 0.04417973756790161, Cumulated Time: 92.43111238876979m \n",
      "**************************************************\n",
      "Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9682    0.9921    0.9800       889\n",
      "           1     0.9914    0.9654    0.9782       838\n",
      "\n",
      "    accuracy                         0.9792      1727\n",
      "   macro avg     0.9798    0.9788    0.9791      1727\n",
      "weighted avg     0.9795    0.9792    0.9791      1727\n",
      "\n",
      "Epoch : 2, weighted avg f1-score: 97.914 Acc : 97.915 on Test_set, Spend:4.187 minutes for evaluation\n",
      "**************************************************\n",
      "Epoch:2 Completed, Total Train Loss:12.927220580284484, Test Loss:7.700886099017225, Spend 99.02813796599706m \n",
      "Epoch:3-0/263, Train cross_entropy Loss: 0.009455070830881596, Cumulated Time: 99.19531596501669m \n",
      "Epoch:3-40/263, Train cross_entropy Loss: 0.013845592737197876, Cumulated Time: 103.65462719599405m \n",
      "Epoch:3-80/263, Train cross_entropy Loss: 0.0018522889586165547, Cumulated Time: 108.14053005774817m \n",
      "Epoch:3-120/263, Train cross_entropy Loss: 0.001293614157475531, Cumulated Time: 112.64987491766611m \n",
      "Epoch:3-160/263, Train cross_entropy Loss: 0.0015768306329846382, Cumulated Time: 117.05321045716603m \n",
      "Epoch:3-200/263, Train cross_entropy Loss: 0.0010839345632120967, Cumulated Time: 121.44232811927796m \n",
      "Epoch:3-240/263, Train cross_entropy Loss: 0.0015262450324371457, Cumulated Time: 125.92346656719843m \n",
      "**************************************************\n",
      "Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9909    0.9831    0.9870       889\n",
      "           1     0.9822    0.9905    0.9863       838\n",
      "\n",
      "    accuracy                         0.9867      1727\n",
      "   macro avg     0.9866    0.9868    0.9867      1727\n",
      "weighted avg     0.9867    0.9867    0.9867      1727\n",
      "\n",
      "Epoch : 3, weighted avg f1-score: 98.668 Acc : 98.668 on Test_set, Spend:4.147 minutes for evaluation\n",
      "**************************************************\n",
      "Epoch:3 Completed, Total Train Loss:4.146108788321726, Test Loss:5.857215190597344, Spend 132.49080522855124m \n",
      "Epoch:4-0/263, Train cross_entropy Loss: 0.06616866588592529, Cumulated Time: 132.65894561608633m \n",
      "Epoch:4-40/263, Train cross_entropy Loss: 0.11269067227840424, Cumulated Time: 137.12117887735366m \n",
      "Epoch:4-80/263, Train cross_entropy Loss: 0.0010954038007184863, Cumulated Time: 141.64534093141555m \n",
      "Epoch:4-120/263, Train cross_entropy Loss: 0.0007770020165480673, Cumulated Time: 146.17396732171377m \n",
      "Epoch:4-160/263, Train cross_entropy Loss: 0.0009328678715974092, Cumulated Time: 150.72422838608423m \n",
      "Epoch:4-200/263, Train cross_entropy Loss: 0.0008955817320384085, Cumulated Time: 155.289213291804m \n",
      "Epoch:4-240/263, Train cross_entropy Loss: 0.0007355725392699242, Cumulated Time: 159.86667300860088m \n",
      "**************************************************\n",
      "Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9909    0.9820    0.9864       889\n",
      "           1     0.9811    0.9905    0.9857       838\n",
      "\n",
      "    accuracy                         0.9861      1727\n",
      "   macro avg     0.9860    0.9862    0.9861      1727\n",
      "weighted avg     0.9861    0.9861    0.9861      1727\n",
      "\n",
      "Epoch : 4, weighted avg f1-score: 98.610 Acc : 98.610 on Test_set, Spend:4.287 minutes for evaluation\n",
      "**************************************************\n",
      "Epoch:4 Completed, Total Train Loss:1.4794537997222506, Test Loss:6.513613046583487, Spend 166.633536307017m \n",
      "Epoch:5-0/263, Train cross_entropy Loss: 0.0009851407958194613, Cumulated Time: 166.7570953845978m \n",
      "Epoch:5-40/263, Train cross_entropy Loss: 0.0008073364733718336, Cumulated Time: 171.39018861452737m \n",
      "Epoch:5-80/263, Train cross_entropy Loss: 0.0009246169938705862, Cumulated Time: 176.0304934859276m \n",
      "Epoch:5-120/263, Train cross_entropy Loss: 0.00070404470898211, Cumulated Time: 180.61441729863483m \n",
      "Epoch:5-160/263, Train cross_entropy Loss: 0.0007018053438514471, Cumulated Time: 185.14492972691855m \n",
      "Epoch:5-200/263, Train cross_entropy Loss: 0.0006664860411547124, Cumulated Time: 189.67833199103674m \n",
      "Epoch:5-240/263, Train cross_entropy Loss: 0.0006482995813712478, Cumulated Time: 194.20960963567097m \n",
      "**************************************************\n",
      "Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9898    0.9865    0.9882       889\n",
      "           1     0.9857    0.9893    0.9875       838\n",
      "\n",
      "    accuracy                         0.9878      1727\n",
      "   macro avg     0.9878    0.9879    0.9878      1727\n",
      "weighted avg     0.9878    0.9878    0.9878      1727\n",
      "\n",
      "Epoch : 5, weighted avg f1-score: 98.784 Acc : 98.784 on Test_set, Spend:4.203 minutes for evaluation\n",
      "**************************************************\n",
      "Epoch:5 Completed, Total Train Loss:0.7942091639561113, Test Loss:6.3252873677702155, Spend 200.83611741860707m \n",
      "Epoch:6-0/263, Train cross_entropy Loss: 0.0006786086014471948, Cumulated Time: 201.00538721084595m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:6-40/263, Train cross_entropy Loss: 0.0006751817418262362, Cumulated Time: 205.5134056329727m \n",
      "Epoch:6-80/263, Train cross_entropy Loss: 0.0006185531383380294, Cumulated Time: 210.01118048032126m \n",
      "Epoch:6-120/263, Train cross_entropy Loss: 0.00047780756722204387, Cumulated Time: 214.54984089136124m \n",
      "Epoch:6-160/263, Train cross_entropy Loss: 0.0005299611366353929, Cumulated Time: 219.09166661898294m \n",
      "Epoch:6-200/263, Train cross_entropy Loss: 0.0005592682864516973, Cumulated Time: 223.61491275628407m \n",
      "Epoch:6-240/263, Train cross_entropy Loss: 0.0004873793222941458, Cumulated Time: 228.15534779628118m \n",
      "**************************************************\n",
      "Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9921    0.9831    0.9876       889\n",
      "           1     0.9823    0.9916    0.9869       838\n",
      "\n",
      "    accuracy                         0.9873      1727\n",
      "   macro avg     0.9872    0.9874    0.9873      1727\n",
      "weighted avg     0.9873    0.9873    0.9873      1727\n",
      "\n",
      "Epoch : 6, weighted avg f1-score: 98.726 Acc : 98.726 on Test_set, Spend:4.171 minutes for evaluation\n",
      "**************************************************\n",
      "Epoch:6 Completed, Total Train Loss:0.38202415747218765, Test Loss:7.11956999413087, Spend 234.7685300429662m \n",
      "Epoch:7-0/263, Train cross_entropy Loss: 0.000435068883234635, Cumulated Time: 234.88897725343705m \n",
      "Epoch:7-40/263, Train cross_entropy Loss: 0.0004572755133267492, Cumulated Time: 239.38675516843796m \n",
      "Epoch:7-80/263, Train cross_entropy Loss: 0.0005036210641264915, Cumulated Time: 243.924811844031m \n",
      "Epoch:7-120/263, Train cross_entropy Loss: 0.00037584855454042554, Cumulated Time: 248.47435753742855m \n",
      "Epoch:7-160/263, Train cross_entropy Loss: 0.000517857086379081, Cumulated Time: 253.053504216671m \n",
      "Epoch:7-200/263, Train cross_entropy Loss: 0.0005586508195847273, Cumulated Time: 257.6785751700401m \n",
      "Epoch:7-240/263, Train cross_entropy Loss: 0.0004531244339887053, Cumulated Time: 262.29578793843586m \n",
      "**************************************************\n",
      "Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9920    0.9820    0.9870       889\n",
      "           1     0.9811    0.9916    0.9864       838\n",
      "\n",
      "    accuracy                         0.9867      1727\n",
      "   macro avg     0.9866    0.9868    0.9867      1727\n",
      "weighted avg     0.9867    0.9867    0.9867      1727\n",
      "\n",
      "Epoch : 7, weighted avg f1-score: 98.668 Acc : 98.668 on Test_set, Spend:4.285 minutes for evaluation\n",
      "**************************************************\n",
      "Epoch:7 Completed, Total Train Loss:0.3621692208980676, Test Loss:6.88659337424906, Spend 269.05635873476666m \n",
      "Epoch:8-0/263, Train cross_entropy Loss: 0.0004284213646315038, Cumulated Time: 269.1804688254992m \n",
      "Epoch:8-40/263, Train cross_entropy Loss: 0.0003595110320020467, Cumulated Time: 273.78611003955206m \n",
      "Epoch:8-80/263, Train cross_entropy Loss: 0.00045077354297973216, Cumulated Time: 278.3644949277242m \n",
      "Epoch:8-120/263, Train cross_entropy Loss: 0.0003615310706663877, Cumulated Time: 282.9580849925677m \n",
      "Epoch:8-160/263, Train cross_entropy Loss: 0.0004385602369438857, Cumulated Time: 287.5270876725515m \n",
      "Epoch:8-200/263, Train cross_entropy Loss: 0.00042277798638679087, Cumulated Time: 292.0661046385765m \n",
      "Epoch:8-240/263, Train cross_entropy Loss: 0.00046746499720029533, Cumulated Time: 296.63280838330587m \n",
      "**************************************************\n",
      "Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9910    0.9854    0.9882       889\n",
      "           1     0.9846    0.9905    0.9875       838\n",
      "\n",
      "    accuracy                         0.9878      1727\n",
      "   macro avg     0.9878    0.9879    0.9878      1727\n",
      "weighted avg     0.9879    0.9878    0.9878      1727\n",
      "\n",
      "Epoch : 8, weighted avg f1-score: 98.784 Acc : 98.784 on Test_set, Spend:4.196 minutes for evaluation\n",
      "**************************************************\n",
      "Epoch:8 Completed, Total Train Loss:0.15630818725912832, Test Loss:6.7604088254011, Spend 303.2739498178164m \n"
     ]
    }
   ],
   "source": [
    "optimizer = BertAdam(model.parameters(), lr=config_learning_rate0,\n",
    "                     warmup=config_warmup_proportion, t_total=total_train_steps, weight_decay=config_l2_decay)\n",
    "\n",
    "train_start = time.time()\n",
    "global_step_th = int(len(train_examples) / batch_size /\n",
    "                     gradient_accumulation_steps * start_epoch)\n",
    "\n",
    "all_loss_list = {'train': [], 'test': []}\n",
    "all_f1_list = {'train': [], 'test': []}\n",
    "for epoch in range(start_epoch, total_train_epochs):\n",
    "    train_loss = 0\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        if prev_save_step > -1:\n",
    "            if step <= prev_save_step:\n",
    "                continue\n",
    "        if prev_save_step > -1:\n",
    "            prev_save_step = -1\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        input_ids, input_mask, segment_ids, y_prob, label_ids, gcn_swop_eye = batch\n",
    "\n",
    "        _, _, _, logits = model(gcn_adj_list, gcn_swop_eye,\n",
    "                          input_ids, segment_ids, input_mask)\n",
    "\n",
    "        if config_loss_criterion == 'mse':\n",
    "            if do_softmax_before_mse:\n",
    "                logits = F.softmax(logits, -1)\n",
    "            loss = F.mse_loss(logits, y_prob)\n",
    "        else:\n",
    "            if loss_weight is None:\n",
    "                loss = F.cross_entropy(logits, label_ids)\n",
    "            else:\n",
    "                loss = F.cross_entropy(\n",
    "                    logits.view(-1, num_classes), label_ids, loss_weight)\n",
    "\n",
    "        if gradient_accumulation_steps > 1:\n",
    "            loss = loss / gradient_accumulation_steps\n",
    "        loss.backward()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        if (step + 1) % gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            global_step_th += 1\n",
    "        if step % 40 == 0:\n",
    "            print(\"Epoch:{}-{}/{}, Train {} Loss: {}, Cumulated Time: {}m \".format(epoch, step,\n",
    "                  len(train_dataloader), config_loss_criterion, loss.item(), (time.time() - train_start)/60.0))\n",
    "\n",
    "    print('*' * 50)\n",
    "    test_loss, test_acc, perform_metrics = evaluate(model, gcn_adj_list, test_dataloader, epoch, 'Test_set')\n",
    "    all_loss_list['train'].append(train_loss)\n",
    "    all_loss_list['test'].append(test_loss)\n",
    "    all_f1_list['test'].append(perform_metrics)\n",
    "    print(\"Epoch:{} Completed, Total Train Loss:{}, Test Loss:{}, Spend {}m \".format(\n",
    "        epoch, train_loss, test_loss, (time.time() - train_start) / 60.0))\n",
    "\n",
    "    if perform_metrics > perform_metrics_prev:\n",
    "        to_save = {'epoch': epoch, 'model_state': model.state_dict(),\n",
    "                   'test_acc': test_acc, 'lower_case': do_lower_case,\n",
    "                   'perform_metrics': perform_metrics}\n",
    "        torch.save(to_save, os.path.join(output_dir, model_save_file))\n",
    "\n",
    "        perform_metrics_prev = perform_metrics\n",
    "\n",
    "        test_f1_best_epoch = epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "059b78a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization Finished! Total Spend Time: 303.3353687286377\n",
      "Test Weighted F1: 98.784 at 8 Epoch.\n"
     ]
    }
   ],
   "source": [
    "print('Optimization Finished! Total Spend Time:', (time.time() - train_start)/60.0)\n",
    "print('Test Weighted F1: %.3f at %d Epoch.' % (100 * perform_metrics_prev, test_f1_best_epoch))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
