{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7902b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import pickle as pkl\n",
    "import re\n",
    "import time\n",
    "import argparse\n",
    "import nltk\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from turtle import window_width\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "from get_data import DataReader\n",
    "from utils import *\n",
    "from pytorch_pretrained_bert import BertTokenizer\n",
    "# from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf8b078",
   "metadata": {},
   "source": [
    "#### Step 1:   Configs for Data Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feaabce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parser = argparse.ArgumentParser()\n",
    "# # ds = dataset, sw = stopwords\n",
    "# parser.add_argument('--ds', type=str, default='pheme')\n",
    "# parser.add_argument('--sw', type=int, default=1)\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# args = {\"ds\": \"pheme\", \"sw\": 1}\n",
    "args = {\"ds\": \"weibo\", \"sw\": 1}\n",
    "# config_dataset = args.ds\n",
    "# config_use_stopwords = True if args.sw else False\n",
    "config_dataset = args[\"ds\"]\n",
    "config_use_stopwords = args[\"sw\"]\n",
    "\n",
    "\n",
    "dump_dir = './prepared_data'\n",
    "if not os.path.exists(dump_dir):\n",
    "    os.mkdir(dump_dir)\n",
    "\n",
    "if config_use_stopwords:\n",
    "    freq_min_for_word_choice = 10\n",
    "else:\n",
    "    freq_min_for_word_choice = 1\n",
    "\n",
    "\n",
    "window_size = 5000  # word co-occurence with context windows, use whole sentence\n",
    "\n",
    "tfidf_mode = 'all_tfidf'  # only_tf / all_tfidf\n",
    "\n",
    "\n",
    "use_tokenizer_at_clean_string = True\n",
    "\n",
    "if config_dataset == \"pheme\":\n",
    "    bert_model_scale = 'bert-base-uncased' # bert-base-uncased / bert-large-uncased\n",
    "elif config_dataset == \"weibo\":\n",
    "    bert_model_scale = 'bert-base-chinese'\n",
    "\n",
    "bert_lower_case = True\n",
    "\n",
    "print('-----------STEP 1: CONFIGS FOR DATA PREPARE-----------')\n",
    "print('Dataset: ', config_dataset.upper())\n",
    "print('Min Frequency for Word Choice: ', freq_min_for_word_choice)\n",
    "print('Window Size: ', window_size)\n",
    "print('Will Delete Stop Words: ', config_use_stopwords,)\n",
    "print('Will Use Bert Tokenizer at Clean: ', use_tokenizer_at_clean_string)\n",
    "print('TF-IDF Mode: ', tfidf_mode)\n",
    "print('Bert Model Scale: ', bert_model_scale)\n",
    "print('Bert Lower Case: ', bert_lower_case)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1948b05e",
   "metadata": {},
   "source": [
    "#### Step 2.1:   Get Texts, Lables, Confidence from Data File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1dde2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-----------STEP 2: GET TWEETS, LABELS, CONFIDENCE FROM DATA FILE-----------')\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "if config_dataset == 'pheme':\n",
    "    train, test = DataReader(\"data/PHEME-SEG/train.txt\", \"./data/PHEME-SEG/test.txt\").read()\n",
    "elif config_dataset == 'weibo':\n",
    "    train, test = DataReader(\"data/WEIBO-SEG/train.txt\", \"./data/WEIBO-SEG/test.txt\").read()\n",
    "\n",
    "train_size = len(train)\n",
    "test_size = len(test)\n",
    "print('%s Dataset, train_size: %d, test_size: %d' \n",
    "      % (config_dataset.upper(), train_size, test_size))\n",
    "\n",
    "trainset = {}\n",
    "testset = {}\n",
    "\n",
    "for data, dataset in [(train, trainset), (test, testset)]:\n",
    "    label = []\n",
    "    all_text = []\n",
    "    for line in data:\n",
    "        label.append(line[0])\n",
    "        all_text.append(line[1])\n",
    "    dataset[\"label\"] = label\n",
    "    dataset[\"data\"] = all_text\n",
    "\n",
    "label_to_index = {label: i for i, label in enumerate(testset['label'])}\n",
    "index_to_label = {i: label for i, label in enumerate(testset['label'])}\n",
    "\n",
    "corpus = trainset['data'] + testset['data']\n",
    "label = np.array(trainset['label'] + testset['label'])\n",
    "corpus_size = len(corpus)\n",
    "label_prob = np.eye(corpus_size, len(label_to_index))[label]\n",
    "\n",
    "doc_content_list = []\n",
    "for t in corpus:\n",
    "    doc_content_list.append(del_http_user_tokenize(t))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5803ecc",
   "metadata": {},
   "source": [
    "#### Step 2.2:   Get Statistics for Original Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70bee17",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len_seq = 0\n",
    "max_len_seq_idx = -1\n",
    "min_len_seq = 1000\n",
    "min_len_seq_idx = -1\n",
    "sen_len_list = []\n",
    "\n",
    "for i, seq in enumerate(doc_content_list):\n",
    "    if config_dataset == 'pheme':\n",
    "        seq = seq.split()\n",
    "    \n",
    "    sen_len_list.append(len(seq))\n",
    "    if len(seq) < min_len_seq:\n",
    "        min_len_seq = len(seq)\n",
    "        min_len_seq_idx = i\n",
    "    if len(seq) > max_len_seq:\n",
    "        max_len_seq = len(seq)\n",
    "        max_len_seq_idx = i\n",
    "\n",
    "print('Statistics for original text')\n",
    "print('max_len: %d, max_len_id: %d, min_len: %d, min_len_id: %d, avg_len: %.2f'\n",
    "      % (max_len_seq, max_len_seq_idx, min_len_seq, min_len_seq_idx, np.array(sen_len_list).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53f0648",
   "metadata": {},
   "source": [
    "#### Step 3.1:   Remove Stopwords from Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b7aeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-----------STEP 3: TOKENIZE SENTENCES & REMOVE STOP WORDS FROM TEXTS-----------')\n",
    "\n",
    "if config_use_stopwords and config_dataset == 'pheme':\n",
    "    \n",
    "    from nltk.corpus import stopwords\n",
    "#     nltk.download('stopwords')\n",
    "    stop_words = stopwords.words('english')\n",
    "    stop_words = set(stop_words)\n",
    "else:\n",
    "    stop_words = {}\n",
    "print('Stop words:', stop_words)\n",
    "\n",
    "\n",
    "tmp_word_freq = {}  # to remove rare words\n",
    "new_doc_content_list = []\n",
    "\n",
    "# use bert_tokenizer for split the sentence\n",
    "if use_tokenizer_at_clean_string:\n",
    "    print('Use bert_tokenizer for seperate words to bert vocab')\n",
    "    bert_tokenizer = BertTokenizer.from_pretrained(\n",
    "        bert_model_scale, do_lower_case=bert_lower_case)\n",
    "\n",
    "for doc_content in tqdm(doc_content_list, desc=\"Tokenize Texts\", colour='green'):\n",
    "    if config_dataset == 'pheme':\n",
    "        new_doc = clean_str(doc_content)\n",
    "\n",
    "    if use_tokenizer_at_clean_string:\n",
    "        if config_dataset == 'pheme':\n",
    "            sub_words = bert_tokenizer.tokenize(new_doc)\n",
    "        elif config_dataset == 'weibo':\n",
    "            sub_words = bert_tokenizer.tokenize(doc_content)\n",
    "        sub_doc = ' '.join(sub_words).strip()\n",
    "        new_doc = sub_doc\n",
    "    new_doc_content_list.append(new_doc)\n",
    "\n",
    "    for word in new_doc.split():\n",
    "        if word in tmp_word_freq:\n",
    "            tmp_word_freq[word] += 1\n",
    "        else:\n",
    "            tmp_word_freq[word] = 1\n",
    "\n",
    "doc_content_list = new_doc_content_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b4cab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_docs = []\n",
    "count_void_doc = 0\n",
    "\n",
    "for i, doc_content in enumerate(doc_content_list):\n",
    "    words = doc_content.split()\n",
    "    doc_words = []\n",
    "\n",
    "    for word in words:\n",
    "        if word not in stop_words and tmp_word_freq[word] >= freq_min_for_word_choice:\n",
    "            doc_words.append(word)\n",
    "\n",
    "    doc_str = ' '.join(doc_words).strip()\n",
    "\n",
    "    if doc_str == '':\n",
    "        count_void_doc += 1\n",
    "        print('No.', i, 'is a empty doc after treat, replaced by \\'%s\\'. original: %s' % (\n",
    "            doc_str, doc_content))\n",
    "    clean_docs.append(doc_str)\n",
    "\n",
    "\n",
    "print('Total', count_void_doc, ' docs are empty.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d0c8ec",
   "metadata": {},
   "source": [
    "#### Step 3.2:   Get Statistics for Spilt and Cleaned Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c13918",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_len = 10000\n",
    "min_len_id = -1\n",
    "max_len = 0\n",
    "max_len_id = -1\n",
    "aver_len = 0\n",
    "\n",
    "for i, line in enumerate(clean_docs):\n",
    "    temp = line.strip().split()\n",
    "    aver_len = aver_len + len(temp)\n",
    "    if len(temp) < min_len:\n",
    "        min_len = len(temp)\n",
    "        min_len_id = i\n",
    "    if len(temp) > max_len:\n",
    "        max_len = len(temp)\n",
    "        max_len_id = i\n",
    "\n",
    "aver_len = 1.0 * aver_len / len(clean_docs)\n",
    "print('Statistics after stopwords and tokenizer:')\n",
    "print('min_len : ' + str(min_len) + ', min_len_id: ' + str(min_len_id))\n",
    "print('max_len : ' + str(max_len) + ', max_len_id: ' + str(max_len_id))\n",
    "print('average_len : ' + str(aver_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb3ea9e",
   "metadata": {},
   "source": [
    "#### Step 4.1:   Prepare Data for Build Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c778faf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_docs = clean_docs[: train_size]\n",
    "# test_docs = clean_docs[train_size : train_size + test_size]\n",
    "\n",
    "train_label = label[: train_size]\n",
    "test_label = label[train_size: train_size + test_size]\n",
    "\n",
    "train_label_prob = label_prob[: train_size]\n",
    "test_label_prob = label_prob[train_size: train_size + test_size]\n",
    "\n",
    "# build vocab using whole corpus(train + test + genelization)\n",
    "word_set = set()\n",
    "for doc_words in clean_docs:\n",
    "    words = doc_words.split()\n",
    "    for word in words:\n",
    "        word_set.add(word)\n",
    "\n",
    "vocab = list(word_set)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "vocab_map = {}\n",
    "for i in range(vocab_size):\n",
    "    vocab_map[vocab[i]] = i\n",
    "\n",
    "word_doc_list = {}\n",
    "for i in range(len(clean_docs)):\n",
    "    doc_words = clean_docs[i]\n",
    "    words = doc_words.split()\n",
    "    appeared = set()\n",
    "    for word in words:\n",
    "        if word in appeared:\n",
    "            continue\n",
    "        if word in word_doc_list:\n",
    "            doc_list = word_doc_list[word]\n",
    "            doc_list.append(i)\n",
    "            word_doc_list[word] = doc_list\n",
    "        else:\n",
    "            word_doc_list[word] = [i]\n",
    "        appeared.add(word)\n",
    "\n",
    "word_doc_freq = {}\n",
    "for word, doc_list in word_doc_list.items():\n",
    "    word_doc_freq[word] = len(doc_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb8ecf3",
   "metadata": {},
   "source": [
    "#### Step 4.2:   Build Document-Word Heterogeneous Graph and Vocabulary Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db704f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('----------STEP 4: BUILD GRAPH----------')\n",
    "\n",
    "print('Calculate first isomerous adj and first isomorphic vocab adj, get word-word PMI values')\n",
    "\n",
    "adj_label = np.hstack((train_label, np.zeros(vocab_size), test_label))\n",
    "adj_label_prob = np.vstack((train_label_prob, np.zeros((vocab_size, len(\n",
    "    label_to_index)), dtype=np.float32), test_label_prob))\n",
    "\n",
    "windows = []\n",
    "for doc_words in clean_docs:\n",
    "    words = doc_words.split()\n",
    "    length = len(words)\n",
    "    if length <= window_size:\n",
    "        windows.append(words)\n",
    "    else:\n",
    "        for j in range(length - window_size + 1):\n",
    "            window = words[j: j + window_size]\n",
    "            windows.append(window)\n",
    "\n",
    "print('Train_valid size:', len(clean_docs), ', Window number:', len(windows))\n",
    "\n",
    "word_window_freq = {}\n",
    "for window in windows:\n",
    "    appeared = set()\n",
    "    for i in range(len(window)):\n",
    "        if window[i] in appeared:\n",
    "            continue\n",
    "        if window[i] in word_window_freq:\n",
    "            word_window_freq[window[i]] += 1\n",
    "        else:\n",
    "            word_window_freq[window[i]] = 1\n",
    "        appeared.add(window[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652ca074",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_pair_count = {}\n",
    "\n",
    "for window in tqdm(windows, desc=\"Word Cooccurence within Windows\", colour='green'):\n",
    "    appeared = set()\n",
    "\n",
    "    for i in range(1, len(window)):\n",
    "        for j in range(0, i):\n",
    "            word_i = window[i]\n",
    "            word_i_id = vocab_map[word_i]\n",
    "            word_j = window[j]\n",
    "            word_j_id = vocab_map[word_j]\n",
    "\n",
    "            if word_i_id == word_j_id:\n",
    "                continue\n",
    "            word_pair_str = str(word_i_id) + ',' + str(word_j_id)\n",
    "            if word_pair_str in appeared:\n",
    "                continue\n",
    "            if word_pair_str in word_pair_count:\n",
    "                word_pair_count[word_pair_str] += 1\n",
    "            else:\n",
    "                word_pair_count[word_pair_str] = 1\n",
    "            appeared.add(word_pair_str)\n",
    "            # two orders\n",
    "            word_pair_str = str(word_j_id) + ',' + str(word_i_id)\n",
    "            if word_pair_str in appeared:\n",
    "                continue\n",
    "            if word_pair_str in word_pair_count:\n",
    "                word_pair_count[word_pair_str] += 1\n",
    "            else:\n",
    "                word_pair_count[word_pair_str] = 1\n",
    "            appeared.add(word_pair_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0569539e",
   "metadata": {},
   "outputs": [],
   "source": [
    "row = []\n",
    "col = []\n",
    "weight = []\n",
    "tfidf_row = []\n",
    "tfidf_col = []\n",
    "tfidf_weight = []\n",
    "vocab_adj_row = []\n",
    "vocab_adj_col = []\n",
    "vocab_adj_weight = []\n",
    "\n",
    "num_window = len(windows)\n",
    "tmp_max_npmi = 0\n",
    "tmp_min_npmi = 0\n",
    "tmp_max_pmi = 0\n",
    "tmp_min_pmi = 0\n",
    "\n",
    "for key in tqdm(word_pair_count, desc=\"Counting PMI and NPMI: \", colour='green'):\n",
    "    temp = key.split(',')\n",
    "    i = int(temp[0])\n",
    "    j = int(temp[1])\n",
    "    count = word_pair_count[key]\n",
    "    word_freq_i = word_window_freq[vocab[i]]\n",
    "    word_freq_j = word_window_freq[vocab[j]]\n",
    "\n",
    "    pmi = log((1.0 * count / num_window) /\n",
    "              (1.0 * word_freq_i * word_freq_j / (num_window * num_window)))\n",
    "\n",
    "    npmi = log(1.0 * word_freq_i * word_freq_j / (num_window *\n",
    "               num_window)) / log(1.0 * count / num_window) - 1\n",
    "\n",
    "    if npmi > tmp_max_npmi:\n",
    "        tmp_max_npmi = npmi\n",
    "    if npmi < tmp_min_npmi:\n",
    "        tmp_min_npmi = npmi\n",
    "    if pmi > tmp_max_pmi:\n",
    "        tmp_max_pmi = pmi\n",
    "    if pmi < tmp_min_pmi:\n",
    "        tmp_min_pmi = pmi\n",
    "    if pmi > 0:\n",
    "        row.append(train_size + i)\n",
    "        col.append(train_size + j)\n",
    "        weight.append(pmi)\n",
    "    if npmi > 0:\n",
    "        vocab_adj_row.append(i)\n",
    "        vocab_adj_col.append(j)\n",
    "        vocab_adj_weight.append(npmi)\n",
    "\n",
    "print('max_pmi:', tmp_max_pmi, 'min_pmi:', tmp_min_pmi)\n",
    "print('max_npmi:', tmp_max_npmi, 'min_npmi:', tmp_min_npmi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb4d113",
   "metadata": {},
   "source": [
    "#### Step 5.1:   Calculate Document-Word TF-IDF Weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8ffcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_docs = len(clean_docs)\n",
    "doc_word_freq = {}\n",
    "for doc_id in range(n_docs):\n",
    "    doc_words = clean_docs[doc_id]\n",
    "    words = doc_words.split()\n",
    "    for word in words:\n",
    "        word_id = vocab_map[word]\n",
    "        doc_word_str = str(doc_id) + ',' + str(word_id)\n",
    "        if doc_word_str in doc_word_freq:\n",
    "            doc_word_freq[doc_word_str] += 1\n",
    "        else:\n",
    "            doc_word_freq[doc_word_str] = 1\n",
    "\n",
    "for i in range(n_docs):\n",
    "    doc_words = clean_docs[i]\n",
    "    words = doc_words.split()\n",
    "    doc_word_set = set()\n",
    "    tfidf_vec = []\n",
    "    for word in words:\n",
    "        if word in doc_word_set:\n",
    "            continue\n",
    "        j = vocab_map[word]\n",
    "        key = str(i) + ',' + str(j)\n",
    "        tf = doc_word_freq[key]\n",
    "        tfidf_row.append(i)\n",
    "        if i < train_size:\n",
    "            row.append(i)\n",
    "        else:\n",
    "            row.append(i + vocab_size)\n",
    "        tfidf_col.append(j)\n",
    "        col.append(train_size + j)\n",
    "        # smooth\n",
    "        idf = log((1.0 + n_docs) / (1.0+word_doc_freq[vocab[j]])) + 1.0\n",
    "        # weight.append(tf * idf)\n",
    "        if tfidf_mode == 'only_tf':\n",
    "            tfidf_vec.append(tf)\n",
    "        else:\n",
    "            tfidf_vec.append(tf * idf)\n",
    "        doc_word_set.add(word)\n",
    "    if len(tfidf_vec) > 0:\n",
    "        weight.extend(tfidf_vec)\n",
    "        tfidf_weight.extend(tfidf_vec)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a80423",
   "metadata": {},
   "source": [
    "#### Step 5.2:   Assemble Adjacency Matrix and Dump to Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79599e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('----------STEP 5: ASSEMBLE ADJACENCY MATRIX AND DUMP TO FILES----------')\n",
    "\n",
    "node_size = vocab_size + corpus_size\n",
    "\n",
    "adj_list = []\n",
    "adj_list.append(sp.csr_matrix((weight, (row, col)),\n",
    "                shape=(node_size, node_size), dtype=np.float32))\n",
    "for i, adj in enumerate(adj_list):\n",
    "    adj_list[i] = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "    adj_list[i].setdiag(1.0)\n",
    "\n",
    "vocab_adj = sp.csr_matrix((vocab_adj_weight, (vocab_adj_row, vocab_adj_col)), shape=(\n",
    "    vocab_size, vocab_size), dtype=np.float32)\n",
    "vocab_adj.setdiag(1.0)\n",
    "\n",
    "print('Calculate isomorphic vocab adjacency matrix using doc\\'s tf-idf...')\n",
    "tfidf_all = sp.csr_matrix((tfidf_weight, (tfidf_row, tfidf_col)), shape=(\n",
    "    corpus_size, vocab_size), dtype=np.float32)\n",
    "tfidf_train = tfidf_all[:train_size]\n",
    "tfidf_test = tfidf_all[train_size:train_size + test_size]\n",
    "\n",
    "tfidf_X_list = [tfidf_train, tfidf_test]\n",
    "vocab_tfidf = tfidf_all.T.tolil()\n",
    "for i in range(vocab_size):\n",
    "    norm = np.linalg.norm(vocab_tfidf.data[i])\n",
    "    if norm > 0:\n",
    "        vocab_tfidf.data[i] /= norm\n",
    "vocab_adj_tf = vocab_tfidf.dot(vocab_tfidf.T)\n",
    "\n",
    "# check\n",
    "print('Check adjacent matrix...')\n",
    "for k in range(len(adj_list)):\n",
    "    count = 0\n",
    "    for i in range(adj_list[k].shape[0]):\n",
    "        if adj_list[k][i, i] <= 0:\n",
    "            count += 1\n",
    "            print('No.%d adj, abnormal diagonal found, No.%d' % (k, i))\n",
    "    if count > 0:\n",
    "        print('No.%d adj, total %d zero diagonal found.' % (k, count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289e6610",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Dump objects...')\n",
    "\n",
    "with open(dump_dir+\"/data_%s.index_label\" % config_dataset, 'wb') as f:\n",
    "    pkl.dump([label_to_index, index_to_label], f)\n",
    "\n",
    "with open(dump_dir+\"/data_%s.vocab_map\" % config_dataset, 'wb') as f:\n",
    "    pkl.dump(vocab_map, f)\n",
    "\n",
    "with open(dump_dir+\"/data_%s.vocab\" % config_dataset, 'wb') as f:\n",
    "    pkl.dump(vocab, f)\n",
    "\n",
    "with open(dump_dir+\"/data_%s.adj_list\" % config_dataset, 'wb') as f:\n",
    "    pkl.dump(adj_list, f)\n",
    "with open(dump_dir+\"/data_%s.label\" % config_dataset, 'wb') as f:\n",
    "    pkl.dump(label, f)\n",
    "with open(dump_dir+\"/data_%s.label_prob\" % config_dataset, 'wb') as f:\n",
    "    pkl.dump(label_prob, f)\n",
    "with open(dump_dir+\"/data_%s.train_label\" % config_dataset, 'wb') as f:\n",
    "    pkl.dump(train_label, f)\n",
    "with open(dump_dir+\"/data_%s.train_label_prob\" % config_dataset, 'wb') as f:\n",
    "    pkl.dump(train_label_prob, f)\n",
    "with open(dump_dir+\"/data_%s.test_label\" % config_dataset, 'wb') as f:\n",
    "    pkl.dump(test_label, f)\n",
    "with open(dump_dir+\"/data_%s.test_label_prob\" % config_dataset, 'wb') as f:\n",
    "    pkl.dump(test_label_prob, f)\n",
    "with open(dump_dir+\"/data_%s.tfidf_list\" % config_dataset, 'wb') as f:\n",
    "    pkl.dump(tfidf_X_list, f)\n",
    "with open(dump_dir+\"/data_%s.vocab_adj_pmi\" % (config_dataset), 'wb') as f:\n",
    "    pkl.dump(vocab_adj, f)\n",
    "with open(dump_dir+\"/data_%s.vocab_adj_tf\" % (config_dataset), 'wb') as f:\n",
    "    pkl.dump(vocab_adj_tf, f)\n",
    "with open(dump_dir+\"/data_%s.clean_docs\" % config_dataset, 'wb') as f:\n",
    "    pkl.dump(clean_docs, f)\n",
    "\n",
    "print('Data prepared, spend %.2f s' % (time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b647fe00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
