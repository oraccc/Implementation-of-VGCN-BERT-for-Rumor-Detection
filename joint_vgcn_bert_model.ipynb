{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfba28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import argparse\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from pytorch_pretrained_bert.modeling import BertModel, BertConfig, WEIGHTS_NAME, CONFIG_NAME\n",
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "from pytorch_pretrained_bert.optimization import BertAdam  # , warmup_linear\n",
    "\n",
    "\n",
    "from utils import *\n",
    "from model_vgcn_bert import VGCN_Bert\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54f7655",
   "metadata": {},
   "source": [
    "#### Step 1:   Configurations for Evaluating VGCN_BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ad4e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\"ds\": \"weibo\", \"load\": 1, \"sw\": 1, \"dim\": 16,\n",
    "        \"lr\": 1e-5, \"l2\": 0.01, \"model\": \"VGCN_BERT\"}\n",
    "\n",
    "config_dataset = args[\"ds\"]\n",
    "config_load_model_from_checkpoint = True if args[\"load\"] == 1 else False\n",
    "config_use_stopwords = True if args[\"sw\"] == 1 else False\n",
    "config_gcn_embedding_dim = args[\"dim\"]\n",
    "config_learning_rate0 = args[\"lr\"]\n",
    "config_l2_decay = args[\"l2\"]\n",
    "config_model_type = args[\"model\"]\n",
    "\n",
    "config_warmup_proportion = 0.1\n",
    "config_vocab_adj = 'all'  # pmi / tf / all\n",
    "config_adj_npmi_threshold = 0.2\n",
    "config_adj_tf_threshold = 0\n",
    "config_loss_criterion = 'cross_entropy'\n",
    "\n",
    "MAX_SEQ_LENGTH = 200 + config_gcn_embedding_dim\n",
    "total_train_epochs = 9\n",
    "batch_size = 16  # 12\n",
    "gradient_accumulation_steps = 1\n",
    "if config_dataset == 'pheme':\n",
    "    bert_model_scale = 'bert-base-uncased'\n",
    "elif config_dataset == 'weibo':\n",
    "    bert_model_scale = 'bert-base-chinese'\n",
    "\n",
    "do_lower_case = True\n",
    "perform_metrics_str = ['weighted avg', 'f1-score']\n",
    "do_softmax_before_mse = True\n",
    "\n",
    "data_dir = './prepared_data/'\n",
    "output_dir = './model_output/'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "\n",
    "\n",
    "model_file_save = config_model_type + str(config_gcn_embedding_dim) + '_model_' + \\\n",
    "    config_dataset + '_' + config_loss_criterion + '_' + \\\n",
    "    \"sw\" + str(int(config_use_stopwords)) + '.pt'\n",
    "\n",
    "print('----------STEP 1: CONFIGURATIONS FOR TRAINING--------')\n",
    "print('Dataset: ', config_dataset)\n",
    "print('Will Load Model from Checkpoint: ', config_load_model_from_checkpoint)\n",
    "print('Will Delete Stop Words: ', config_use_stopwords)\n",
    "print('Vocab GCN Hidden Dim: vocab_size -> 128 -> ' + str(config_gcn_embedding_dim))\n",
    "print('Learning Rate0: ', config_learning_rate0)\n",
    "print('Weight Decay: ', config_l2_decay)\n",
    "print('Loss Criterion: ', config_loss_criterion)\n",
    "print('Will Perform Softmax before MSE: ', do_softmax_before_mse)\n",
    "print('Vocab Adjcent: ', config_vocab_adj)\n",
    "print('MAX_SEQ_LENGTH: ', MAX_SEQ_LENGTH)\n",
    "print('Perform Metrics: ', perform_metrics_str)\n",
    "print('Load Model File Name: ', model_file_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79eede6",
   "metadata": {},
   "source": [
    "#### Step 2.1: Prepare Dataset & Load Vocabulary Adjacent Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0e7e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('----------STEP 2: PREPARE DATASET & LOAD VOCABULARY ADJACENT MATRIX----------')\n",
    "print(' Load and seperate', config_dataset, 'dataset, with vocabulary graph adjacent matrix')\n",
    "\n",
    "objects = []\n",
    "names = ['index_label', 'train_label', 'train_label_prob', 'test_label',\n",
    "         'test_label_prob', 'clean_docs', 'vocab_adj_tf', 'vocab_adj_pmi', 'vocab_map']\n",
    "\n",
    "for i in range(len(names)):\n",
    "    datafile = data_dir + \"/data_%s.%s\" % (config_dataset, names[i])\n",
    "    with open(datafile, 'rb') as f:\n",
    "        objects.append(pkl.load(f, encoding='latin1'))\n",
    "\n",
    "index_labels_list, train_label, train_label_prob, test_label, test_label_prob, shuffled_clean_docs, gcn_vocab_adj_tf, gcn_vocab_adj_pmi, gcn_vocab_map = tuple(objects)\n",
    "\n",
    "label2idx = index_labels_list[0]\n",
    "idx2label = index_labels_list[1]\n",
    "\n",
    "all_labels = np.hstack((train_label, test_label))\n",
    "all_labels_prob = np.vstack((train_label_prob, test_label_prob))\n",
    "\n",
    "examples = []\n",
    "for i, text in enumerate(shuffled_clean_docs):\n",
    "    example = InputExample(i, text.strip(), confidence=all_labels_prob[i], label=all_labels[i])\n",
    "    examples.append(example)\n",
    "\n",
    "num_classes = len(label2idx)\n",
    "gcn_vocab_size = len(gcn_vocab_map)\n",
    "train_size = len(train_label)\n",
    "test_size = len(test_label)\n",
    "\n",
    "indexs = np.arange(0, len(examples))\n",
    "train_examples = [examples[i] for i in indexs[:train_size]]\n",
    "test_examples = [examples[i] for i in indexs[train_size:train_size + test_size]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa16c2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config_adj_tf_threshold > 0:\n",
    "    gcn_vocab_adj_tf.data *= (gcn_vocab_adj_tf.data > config_adj_tf_threshold)\n",
    "    gcn_vocab_adj_tf.eliminate_zeros()\n",
    "if config_adj_npmi_threshold > 0:\n",
    "    gcn_vocab_adj_pmi.data *= (gcn_vocab_adj_pmi.data > config_adj_npmi_threshold)\n",
    "    gcn_vocab_adj_pmi.eliminate_zeros()\n",
    "\n",
    "if config_vocab_adj == 'pmi':\n",
    "    gcn_vocab_adj_list = [gcn_vocab_adj_pmi]\n",
    "elif config_vocab_adj == 'tf':\n",
    "    gcn_vocab_adj_list = [gcn_vocab_adj_tf]\n",
    "elif config_vocab_adj == 'all':\n",
    "    gcn_vocab_adj_list = [gcn_vocab_adj_tf, gcn_vocab_adj_pmi]\n",
    "\n",
    "norm_gcn_vocab_adj_list = []\n",
    "for i in range(len(gcn_vocab_adj_list)):\n",
    "    adj = gcn_vocab_adj_list[i]\n",
    "\n",
    "    print('Zero ratio for vocab adj %dth: %.8f' %\n",
    "          (i, 100 * (1 - adj.count_nonzero() / (adj.shape[0] * adj.shape[1]))))\n",
    "\n",
    "    adj = normalize_adj(adj)\n",
    "    norm_gcn_vocab_adj_list.append(sparse_scipy2torch(adj.tocoo()).to(device))\n",
    "\n",
    "gcn_adj_list = norm_gcn_vocab_adj_list\n",
    "\n",
    "\n",
    "train_classes_num, train_classes_weight = get_class_count_and_weight(train_label, len(label2idx))\n",
    "loss_weight = torch.tensor(train_classes_weight).to(device)\n",
    "loss_weight = torch.tensor(loss_weight, dtype=torch.float32).to(device)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model_scale, do_lower_case=do_lower_case)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac765aa4",
   "metadata": {},
   "source": [
    "#### Step 2.2:   Prepare PyTorch DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916e2c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pytorch_dataloader(examples, tokenizer, batch_size):\n",
    "    dataset = CorpusDataset(examples, tokenizer, gcn_vocab_map, MAX_SEQ_LENGTH, config_gcn_embedding_dim)\n",
    "    return DataLoader(dataset=dataset, batch_size=batch_size, shuffle=False, num_workers=4, collate_fn=dataset.pad)\n",
    "\n",
    "\n",
    "train_dataloader = get_pytorch_dataloader(train_examples, tokenizer, batch_size)\n",
    "test_dataloader = get_pytorch_dataloader(test_examples, tokenizer, batch_size)\n",
    "\n",
    "total_train_steps = int(len(train_dataloader) / gradient_accumulation_steps * total_train_epochs)\n",
    "\n",
    "print('Train Classes Count: ', train_classes_num)\n",
    "print('Batch size: ', batch_size)\n",
    "print('Num steps: ', total_train_steps)\n",
    "print('Number of Examples for Training: ', len(train_examples))\n",
    "print('Number of Examples for Training After Dataloader: ', len(train_dataloader) * batch_size)\n",
    "print('Number of Examples for Validate: ', len(test_examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df124ae",
   "metadata": {},
   "source": [
    "#### Step 3.1:   Load Trained VGCN_BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf798ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config_load_model_from_checkpoint and os.path.exists(os.path.join(output_dir, model_file_save)):\n",
    "    checkpoint = torch.load(os.path.join(output_dir, model_file_save), map_location='cpu')\n",
    "    if 'step' in checkpoint:\n",
    "        prev_save_step = checkpoint['step']\n",
    "        start_epoch = checkpoint['epoch']\n",
    "    else:\n",
    "        prev_save_step = -1\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "\n",
    "    valid_acc_prev = checkpoint['valid_acc']\n",
    "    perform_metrics_prev = checkpoint['perform_metrics']\n",
    "    model = VGCN_Bert.from_pretrained(bert_model_scale, state_dict=checkpoint['model_state'], gcn_adj_dim=gcn_vocab_size, \n",
    "        gcn_adj_num=len(gcn_adj_list), gcn_embedding_dim=config_gcn_embedding_dim, num_labels=len(label2idx))\n",
    "\n",
    "    pretrained_dict = checkpoint['model_state']\n",
    "    net_state_dict = model.state_dict()\n",
    "    pretrained_dict_selected = {\n",
    "        k: v for k, v in pretrained_dict.items() if k in net_state_dict}\n",
    "    net_state_dict.update(pretrained_dict_selected)\n",
    "    model.load_state_dict(net_state_dict)\n",
    "\n",
    "    print('Loaded the pretrain model:', model_file_save, ', epoch:', checkpoint['epoch'], 'step:', prev_save_step, 'valid acc:',\n",
    "          checkpoint['valid_acc'], ' '.join(perform_metrics_str)+'_valid:', checkpoint['perform_metrics'])\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1579efee",
   "metadata": {},
   "source": [
    "#### Step 3.2: Evaluate VGCN_BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ef2042",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, gcn_adj_list,predict_dataloader, batch_size):\n",
    "    # print(\"***** Running prediction *****\")\n",
    "    model.eval()\n",
    "    predict_out = []\n",
    "    all_label_ids = []\n",
    "    ev_loss=0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(predict_dataloader, desc=\"Evaluating\", colour='green'):\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            input_ids, input_mask, segment_ids, y_prob, label_ids, gcn_swop_eye = batch\n",
    "            _, logits = model(gcn_adj_list, gcn_swop_eye,input_ids, segment_ids, input_mask)\n",
    "\n",
    "            if config_loss_criterion=='mse':\n",
    "                if do_softmax_before_mse:\n",
    "                    logits=F.softmax(logits,-1)\n",
    "                loss = F.mse_loss(logits, y_prob)\n",
    "            else:\n",
    "                if loss_weight is None:\n",
    "                    loss = F.cross_entropy(logits.view(-1, num_classes), label_ids)\n",
    "                else:\n",
    "                    loss = F.cross_entropy(logits.view(-1, num_classes), label_ids)\n",
    "                    \n",
    "            ev_loss+=loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(logits, -1)\n",
    "            \n",
    "            predict_out.extend(predicted.tolist())\n",
    "            all_label_ids.extend(label_ids.tolist())\n",
    "            eval_accuracy = predicted.eq(label_ids).sum().item()\n",
    "            total += len(label_ids)\n",
    "            correct += eval_accuracy\n",
    "\n",
    "        f1_metrics=f1_score(np.array(all_label_ids).reshape(-1),\n",
    "            np.array(predict_out).reshape(-1), average='weighted')\n",
    "        print(\"Report:\\n\"+classification_report(np.array(all_label_ids).reshape(-1),\n",
    "            np.array(predict_out).reshape(-1),digits=4))\n",
    "\n",
    "    ev_acc = correct / total\n",
    "    return ev_loss, ev_acc, f1_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1343f224",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluate(model, gcn_adj_list, test_dataloader, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0086eb13",
   "metadata": {},
   "source": [
    "#### Step 4.1: Get VGCN_BERT Model Pooled Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af635dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pooled_out(model, gcn_adj_list, predict_dataloader):\n",
    "    \n",
    "    outputs = None\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(predict_dataloader, desc=\"Evaluating\", colour='green'):\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            input_ids, input_mask, segment_ids, y_prob, label_ids, gcn_swop_eye = batch\n",
    "            pooled_output, _= model(gcn_adj_list, gcn_swop_eye, input_ids, segment_ids, input_mask)\n",
    "            \n",
    "            if outputs is None:\n",
    "                outputs = pooled_output.detach().cpu().numpy()\n",
    "            else:\n",
    "                outputs = np.append(outputs, pooled_output.detach().cpu().numpy(), axis=0)\n",
    "    \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0a1cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pooled_outputs = get_pooled_out(model, gcn_adj_list, train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4828bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pooled_outputs = get_pooled_out(model, gcn_adj_list, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95db355",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_pooled_outputs.shape, test_pooled_outputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c461f45",
   "metadata": {},
   "source": [
    "#### Step 4.2: Reorganize Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f1c274",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index_path = './data/' + config_dataset.upper() + '-SEG/train_index_list.txt'\n",
    "test_index_path = './data/' + config_dataset.upper() + '-SEG/test_index_list.txt'\n",
    "\n",
    "train_index_list = []\n",
    "with open(train_index_path, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "    for each_line in lines:\n",
    "        train_index_list.append(int(each_line))\n",
    "        \n",
    "test_index_list = []\n",
    "with open(test_index_path, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "    for each_line in lines:\n",
    "        test_index_list.append(int(each_line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6c4842",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label_path = './data/' + config_dataset.upper() + '-SEG/train_label.txt'\n",
    "test_label_path = './data/' + config_dataset.upper() + '-SEG/test_label.txt'\n",
    "\n",
    "train_label = []\n",
    "with open(train_label_path, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "    for each_line in lines:\n",
    "        train_label.append(int(each_line))\n",
    "\n",
    "test_label = []\n",
    "with open(test_label_path, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "    for each_line in lines:\n",
    "        test_label.append(int(each_line))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5549fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = {}\n",
    "\n",
    "for l, emb in zip(train_index_list,train_pooled_outputs):\n",
    "    if l in train_x.keys():\n",
    "        # np.vstack on lists represents features concatenation \n",
    "        train_x[l]  =np.vstack([train_x[l], emb])\n",
    "    else:\n",
    "        train_x[l] = [emb]\n",
    "\n",
    "train_l_final = []\n",
    "label_l_final = []\n",
    "\n",
    "for k in train_x.keys():\n",
    "    train_l_final.append(train_x[k])\n",
    "    label_l_final.append(train_label[k])\n",
    "\n",
    "df_train = pd.DataFrame({'emb': train_l_final, 'label': label_l_final})\n",
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9292e1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = {}\n",
    "\n",
    "for l, emb in zip(test_index_list,test_pooled_outputs):\n",
    "    if l in test_x.keys():\n",
    "        # np.vstack on lists represents features concatenation \n",
    "        test_x[l]  =np.vstack([test_x[l], emb])\n",
    "    else:\n",
    "        test_x[l] = [emb]\n",
    "\n",
    "test_l_final = []\n",
    "tlabel_l_final = []\n",
    "for k in test_x.keys():\n",
    "    test_l_final.append(test_x[k])\n",
    "    tlabel_l_final.append(test_label[k])\n",
    "\n",
    "df_test = pd.DataFrame({'emb': test_l_final, 'label': tlabel_l_final})\n",
    "df_test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a3d22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_val, df_test = train_test_split(df_test, test_size=0.5, random_state=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811ef558",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train.shape,df_val.shape,df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138c04c4",
   "metadata": {},
   "source": [
    "#### Step 4.3: Generate Data Generator for Joint Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cf37b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_dict = {\n",
    "    \"text_comments\": [[7,663], [3, 232], [5, 93]],\n",
    "    \"pheme_text_comments_refined\": [[23, 227], [6, 179], [6, 179]],\n",
    "    \"weibo_text_comments_refined\": [[3, 1399], [1, 863], [2, 432]],\n",
    "}\n",
    "\n",
    "batch = batch_dict[\"weibo_text_comments_refined\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effcfc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator(df, batch_size, batches_per_epoch):\n",
    "    num_sequences = len(df['emb'].to_list())\n",
    "    assert batch_size * batches_per_epoch == num_sequences\n",
    "    num_features= 768\n",
    "\n",
    "    x_list= df['emb'].to_list()\n",
    "    y_list =  df.label.to_list()\n",
    "    # Generate batches\n",
    "    while True:\n",
    "        for b in range(batches_per_epoch):\n",
    "            longest_index = (b + 1) * batch_size - 1\n",
    "            timesteps = len(max(df['emb'].to_list()[:(b + 1) * batch_size][-batch_size:], key=len))\n",
    "            x_train = np.full((batch_size, timesteps, num_features), -99.)\n",
    "            y_train = np.zeros((batch_size,  1))\n",
    "            for i in range(batch_size):\n",
    "                li = b * batch_size + i\n",
    "                x_train[i, 0:len(x_list[li]), :] = x_list[li]\n",
    "                y_train[i] = y_list[li]\n",
    "            yield x_train, y_train\n",
    "            \n",
    "def val_generator(df,batch_size_val,batches_per_epoch_val):\n",
    "    num_sequences_val = len(df['emb'].to_list())\n",
    "    assert batch_size_val * batches_per_epoch_val == num_sequences_val\n",
    "    num_features= 768\n",
    "\n",
    "    x_list= df['emb'].to_list()\n",
    "    y_list =  df.label.to_list()\n",
    "    # Generate batches\n",
    "    while True:\n",
    "        for b in range(batches_per_epoch_val):\n",
    "            longest_index = (b + 1) * batch_size_val - 1\n",
    "            timesteps = len(max(df['emb'].to_list()[:(b + 1) * batch_size_val][-31:], key=len))\n",
    "            x_val = np.full((batch_size_val, timesteps, num_features), -99.)\n",
    "            y_val = np.zeros((batch_size_val,  1))\n",
    "            for i in range(batch_size_val):\n",
    "                li = b * batch_size_val + i\n",
    "                x_val[i, 0:len(x_list[li]), :] = x_list[li]\n",
    "                y_val[i] = y_list[li]\n",
    "            yield x_val, y_val\n",
    "            \n",
    "def test_generator(df,batch_size_test, batches_per_epoch_test):\n",
    "    num_sequences_test = len(df['emb'].to_list())\n",
    "    assert batch_size_test * batches_per_epoch_test == num_sequences_test\n",
    "    num_features= 768\n",
    "\n",
    "    x_list= df['emb'].to_list()\n",
    "    y_list =  df.label.to_list()\n",
    "    # Generate batches\n",
    "    while True:\n",
    "        for b in range(batches_per_epoch_test):\n",
    "            longest_index = (b + 1) * batch_size_test - 1\n",
    "            timesteps = len(max(df['emb'].to_list()[:(b + 1) * batch_size_test][-31:], key=len))\n",
    "            # print(len(df_train['emb'].to_list()[:b+batch_size][-7:]))\n",
    "            x_test = np.full((batch_size_test, timesteps, num_features), -99.)\n",
    "            y_test = np.zeros((batch_size_test,  1))\n",
    "            for i in range(batch_size_test):\n",
    "                li = b * batch_size_test + i\n",
    "                x_test[i, 0:len(x_list[li]), :] = x_list[li]\n",
    "                y_test[i] = y_list[li]\n",
    "            yield x_test, y_test            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642aa512",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_generator(df_train, batch[0][0], batch[0][1])\n",
    "val_data = val_generator(df_val, batch[1][0], batch[1][1])\n",
    "test_data = test_generator(df_test, batch[2][0], batch[2][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af86ee49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def cul_all_metrics(y_true, y_pred, pos_label=1):\n",
    "    return {\"accuracy\": float(\"%.5f\" % accuracy_score(y_true=y_true, y_pred=y_pred)),\n",
    "            \"precision\": float(\"%.5f\" % precision_score(y_true=y_true, y_pred=y_pred, pos_label=pos_label, average=\"weighted\")),\n",
    "            \"recall\": float(\"%.5f\" % recall_score(y_true=y_true, y_pred=y_pred, pos_label=pos_label, average=\"weighted\")),\n",
    "            \"f1-score\": float(\"%.5f\" % f1_score(y_true=y_true, y_pred=y_pred, average=\"weighted\")),\n",
    "           }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081b520a",
   "metadata": {},
   "source": [
    "#### Step 5: Build \"Recurrence over VGCN_BERT\" Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716ee802",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import h5py\n",
    "\n",
    "text_input = keras.Input(shape=(None,768,), dtype='float32', name='features')\n",
    "l_mask = keras.layers.Masking(mask_value=-99.)(text_input) \n",
    "encoded_text = keras.layers.LSTM(100,)(l_mask)\n",
    "out_dense = keras.layers.Dense(30, activation='relu')(encoded_text)\n",
    "out = keras.layers.Dense(2, activation='softmax')(out_dense)\n",
    "R_Model = keras.Model(text_input, out)\n",
    "R_Model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
    "R_Model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4987216e",
   "metadata": {},
   "outputs": [],
   "source": [
    "call_reduce = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_acc', factor=0.95, patience=3, verbose=2,\n",
    "                                mode='auto', min_delta=0.01, cooldown=0, min_lr=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84358d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_per_epoch = batch[0][1]\n",
    "\n",
    "batches_per_epoch_val= batch[1][1]\n",
    "\n",
    "R_Model.fit(train_data, steps_per_epoch=batches_per_epoch, epochs=10,\n",
    "                    validation_data=val_data, validation_steps=batches_per_epoch_val, callbacks =[call_reduce] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edaee8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_per_epoch_test = batch[2][1]\n",
    "\n",
    "test_data = test_generator(df_test, batch[2][0], batch[2][1])\n",
    "r_score = R_Model.predict_generator(test_data, steps=batches_per_epoch_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb268b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_pred = np.argmax(r_score, axis=1).tolist()\n",
    "label = df_test.label.to_list()\n",
    "\n",
    "cul_all_metrics(label, r_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc669746",
   "metadata": {},
   "source": [
    "#### Step 6: Build \"Transformer over VGCN_BERT\" Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd7cfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads=8):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        assert (\n",
    "            embed_dim % num_heads == 0\n",
    "        ), \"embedding dimension not divisible by num heads\"\n",
    "        self.projection_dim = embed_dim // num_heads\n",
    "        self.wq = keras.layers.Dense(embed_dim)\n",
    "        self.wk = keras.layers.Dense(embed_dim)\n",
    "        self.wv = keras.layers.Dense(embed_dim)\n",
    "        self.combine_heads = keras.layers.Dense(embed_dim)\n",
    "\n",
    "    def attention(self, q, k, v):\n",
    "        score = tf.matmul(q, k, transpose_b=True)\n",
    "        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "        scaled_score = score / tf.math.sqrt(dk)\n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
    "        output = tf.matmul(weights, v)\n",
    "        return output, weights\n",
    "\n",
    "    def separate_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, x):\n",
    "        # x.shape = [batch_size, seq_len, embedding_dim]\n",
    "        batch_size = tf.shape(x)[0]\n",
    "        q = self.wq(x)  # (batch_size, seq_len, embed_dim)\n",
    "        k = self.wk(x)  # (batch_size, seq_len, embed_dim)\n",
    "        v = self.wv(x)  # (batch_size, seq_len, embed_dim)\n",
    "        q = self.separate_heads(\n",
    "            q, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        k = self.separate_heads(\n",
    "            k, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        v = self.separate_heads(\n",
    "            v, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        attention, weights = self.attention(q, k, v)\n",
    "        attention = tf.transpose(\n",
    "            attention, perm=[0, 2, 1, 3]\n",
    "        )  # (batch_size, seq_len, num_heads, projection_dim)\n",
    "        concat_attention = tf.reshape(\n",
    "            attention, (batch_size, -1, self.embed_dim)\n",
    "        )  # (batch_size, seq_len, embed_dim)\n",
    "        output = self.combine_heads(\n",
    "            concat_attention\n",
    "        )  # (batch_size, seq_len, embed_dim)\n",
    "        return output\n",
    "    \n",
    "class TransformerLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerLayer, self).__init__()\n",
    "\n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [\n",
    "                keras.layers.Dense(ff_dim, activation=\"relu\"),\n",
    "                keras.layers.Dense(embed_dim),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training):\n",
    "        attn_output = self.att(x)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5dfb2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "text_input = keras.Input(shape=(None,768,), dtype='float32', name='features')\n",
    "l_mask = keras.layers.Masking(mask_value=-99.)(text_input) \n",
    "transformer_encodings = TransformerLayer(embed_dim=768, num_heads=1, ff_dim=32)(l_mask)\n",
    "encoded_texts = keras.layers.LSTM(100,)(transformer_encodings)\n",
    "out_dense = keras.layers.Dense(30, activation='relu')(encoded_texts)\n",
    "out = keras.layers.Dense(2, activation='softmax')(out_dense)\n",
    "T_Model = keras.Model(text_input, out)\n",
    "T_Model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "T_Model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e1681b",
   "metadata": {},
   "outputs": [],
   "source": [
    "call_reduce = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_acc', factor=0.95, patience=3, verbose=2,\n",
    "                                mode='auto', min_delta=0.01, cooldown=0, min_lr=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35496c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_per_epoch = batch[0][1]\n",
    "\n",
    "batches_per_epoch_val= batch[1][1]\n",
    "\n",
    "T_Model.fit(train_data, steps_per_epoch=batches_per_epoch, epochs=10,\n",
    "                    validation_data=val_data, validation_steps=batches_per_epoch_val, callbacks =[call_reduce] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5768ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_per_epoch_test = batch[2][1]\n",
    "\n",
    "test_data = test_generator(df_test, batch[2][0], batch[2][1])\n",
    "t_score = T_Model.predict_generator(test_data, steps=batches_per_epoch_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ef4707",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_pred = np.argmax(t_score, axis=1).tolist()\n",
    "label = df_test.label.to_list()\n",
    "\n",
    "cul_all_metrics(label, t_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e026654",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "409.6px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
