{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d9e183d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from pytorch_pretrained_bert.modeling import BertModel, BertConfig, WEIGHTS_NAME, CONFIG_NAME\n",
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "from pytorch_pretrained_bert.optimization import BertAdam #, warmup_linear\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from utils import *\n",
    "from model_vgcn_bert import VGCN_Bert\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54f7655",
   "metadata": {},
   "source": [
    "#### Step 1:   Configurations for Evaluating VGCN_BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6ad4e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------STEP 1: CONFIGURATIONS FOR TRAINING--------\n",
      "Dataset:  pheme\n",
      "Will Load Model from Checkpoint:  True\n",
      "Will Delete Stop Words:  True\n",
      "Vocab GCN Hidden Dim: vocab_size -> 128 -> 16\n",
      "Learning Rate0:  1e-05\n",
      "Weight Decay:  0.01\n",
      "Loss Criterion:  cross_entropy\n",
      "Will Perform Softmax before MSE:  True\n",
      "Vocab Adjcent:  all\n",
      "MAX_SEQ_LENGTH:  216\n",
      "Perform Metrics:  ['weighted avg', 'f1-score']\n",
      "Load Model File Name:  VGCN_BERT16_model_pheme_cross_entropy_sw1.pt\n"
     ]
    }
   ],
   "source": [
    "args = {\"ds\": \"pheme\", \"load\": 1, \"sw\": 1, \"dim\": 16,\n",
    "        \"lr\": 1e-5, \"l2\": 0.01, \"model\": \"VGCN_BERT\"}\n",
    "\n",
    "config_dataset = args[\"ds\"]\n",
    "config_load_model_from_checkpoint = True if args[\"load\"] == 1 else False\n",
    "config_use_stopwords = True if args[\"sw\"] == 1 else False\n",
    "config_gcn_embedding_dim = args[\"dim\"]\n",
    "config_learning_rate0 = args[\"lr\"]\n",
    "config_l2_decay = args[\"l2\"]\n",
    "config_model_type = args[\"model\"]\n",
    "\n",
    "config_warmup_proportion = 0.1\n",
    "config_vocab_adj = 'all'  # pmi / tf / all\n",
    "config_adj_npmi_threshold = 0.2\n",
    "config_adj_tf_threshold = 0\n",
    "config_loss_criterion = 'cross_entropy'\n",
    "\n",
    "MAX_SEQ_LENGTH = 200 + config_gcn_embedding_dim\n",
    "total_train_epochs = 9\n",
    "batch_size = 16  # 12\n",
    "gradient_accumulation_steps = 1\n",
    "bert_model_scale = 'bert-base-uncased'\n",
    "do_lower_case = True\n",
    "perform_metrics_str = ['weighted avg', 'f1-score']\n",
    "do_softmax_before_mse = True\n",
    "\n",
    "data_dir = './prepared_data/'\n",
    "output_dir = './model_output/'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "\n",
    "\n",
    "model_file_save = config_model_type + str(config_gcn_embedding_dim) + '_model_' + \\\n",
    "    config_dataset + '_' + config_loss_criterion + '_' + \\\n",
    "    \"sw\" + str(int(config_use_stopwords)) + '.pt'\n",
    "\n",
    "print('----------STEP 1: CONFIGURATIONS FOR TRAINING--------')\n",
    "print('Dataset: ', config_dataset)\n",
    "print('Will Load Model from Checkpoint: ', config_load_model_from_checkpoint)\n",
    "print('Will Delete Stop Words: ', config_use_stopwords)\n",
    "print('Vocab GCN Hidden Dim: vocab_size -> 128 -> ' + str(config_gcn_embedding_dim))\n",
    "print('Learning Rate0: ', config_learning_rate0)\n",
    "print('Weight Decay: ', config_l2_decay)\n",
    "print('Loss Criterion: ', config_loss_criterion)\n",
    "print('Will Perform Softmax before MSE: ', do_softmax_before_mse)\n",
    "print('Vocab Adjcent: ', config_vocab_adj)\n",
    "print('MAX_SEQ_LENGTH: ', MAX_SEQ_LENGTH)\n",
    "print('Perform Metrics: ', perform_metrics_str)\n",
    "print('Load Model File Name: ', model_file_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79eede6",
   "metadata": {},
   "source": [
    "#### Step 2.1: Prepare Dataset & Load Vocabulary Adjacent Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e0e7e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------STEP 2: PREPARE DATASET & LOAD VOCABULARY ADJACENT MATRIX----------\n",
      " Load and seperate pheme dataset, with vocabulary graph adjacent matrix\n"
     ]
    }
   ],
   "source": [
    "print('----------STEP 2: PREPARE DATASET & LOAD VOCABULARY ADJACENT MATRIX----------')\n",
    "print(' Load and seperate', config_dataset, 'dataset, with vocabulary graph adjacent matrix')\n",
    "\n",
    "objects = []\n",
    "names = ['index_label', 'train_label', 'train_label_prob', 'test_label',\n",
    "         'test_label_prob', 'clean_docs', 'vocab_adj_tf', 'vocab_adj_pmi', 'vocab_map']\n",
    "\n",
    "for i in range(len(names)):\n",
    "    datafile = data_dir + \"/data_%s.%s\" % (config_dataset, names[i])\n",
    "    with open(datafile, 'rb') as f:\n",
    "        objects.append(pkl.load(f, encoding='latin1'))\n",
    "\n",
    "index_labels_list, train_label, train_label_prob, test_label, test_label_prob, shuffled_clean_docs, gcn_vocab_adj_tf, gcn_vocab_adj_pmi, gcn_vocab_map = tuple(objects)\n",
    "\n",
    "label2idx = index_labels_list[0]\n",
    "idx2label = index_labels_list[1]\n",
    "\n",
    "all_labels = np.hstack((train_label, test_label))\n",
    "all_labels_prob = np.vstack((train_label_prob, test_label_prob))\n",
    "\n",
    "examples = []\n",
    "for i, text in enumerate(shuffled_clean_docs):\n",
    "    example = InputExample(i, text.strip(), confidence=all_labels_prob[i], label=all_labels[i])\n",
    "    examples.append(example)\n",
    "\n",
    "num_classes = len(label2idx)\n",
    "gcn_vocab_size = len(gcn_vocab_map)\n",
    "train_size = len(train_label)\n",
    "test_size = len(test_label)\n",
    "\n",
    "indexs = np.arange(0, len(examples))\n",
    "train_examples = [examples[i] for i in indexs[:train_size]]\n",
    "test_examples = [examples[i] for i in indexs[train_size:train_size + test_size]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa16c2b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero ratio for vocab adj 0th: 79.40479542\n",
      "Zero ratio for vocab adj 1th: 95.22829647\n"
     ]
    }
   ],
   "source": [
    "if config_adj_tf_threshold > 0:\n",
    "    gcn_vocab_adj_tf.data *= (gcn_vocab_adj_tf.data > config_adj_tf_threshold)\n",
    "    gcn_vocab_adj_tf.eliminate_zeros()\n",
    "if config_adj_npmi_threshold > 0:\n",
    "    gcn_vocab_adj_pmi.data *= (gcn_vocab_adj_pmi.data > config_adj_npmi_threshold)\n",
    "    gcn_vocab_adj_pmi.eliminate_zeros()\n",
    "\n",
    "if config_vocab_adj == 'pmi':\n",
    "    gcn_vocab_adj_list = [gcn_vocab_adj_pmi]\n",
    "elif config_vocab_adj == 'tf':\n",
    "    gcn_vocab_adj_list = [gcn_vocab_adj_tf]\n",
    "elif config_vocab_adj == 'all':\n",
    "    gcn_vocab_adj_list = [gcn_vocab_adj_tf, gcn_vocab_adj_pmi]\n",
    "\n",
    "norm_gcn_vocab_adj_list = []\n",
    "for i in range(len(gcn_vocab_adj_list)):\n",
    "    adj = gcn_vocab_adj_list[i]\n",
    "\n",
    "    print('Zero ratio for vocab adj %dth: %.8f' %\n",
    "          (i, 100 * (1 - adj.count_nonzero() / (adj.shape[0] * adj.shape[1]))))\n",
    "\n",
    "    adj = normalize_adj(adj)\n",
    "    norm_gcn_vocab_adj_list.append(sparse_scipy2torch(adj.tocoo()).to(device))\n",
    "\n",
    "gcn_adj_list = norm_gcn_vocab_adj_list\n",
    "\n",
    "\n",
    "train_classes_num, train_classes_weight = get_class_count_and_weight(train_label, len(label2idx))\n",
    "loss_weight = torch.tensor(train_classes_weight).to(device)\n",
    "loss_weight = torch.tensor(loss_weight, dtype=torch.float32).to(device)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model_scale, do_lower_case=do_lower_case)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac765aa4",
   "metadata": {},
   "source": [
    "#### Step 2.2:   Prepare PyTorch DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "916e2c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Classes Count:  [5341, 2368]\n",
      "Batch size:  16\n",
      "Num steps:  4338\n",
      "Number of Examples for Training:  7709\n",
      "Number of Examples for Training After Dataloader:  7712\n",
      "Number of Examples for Validate:  3174\n"
     ]
    }
   ],
   "source": [
    "def get_pytorch_dataloader(examples, tokenizer, batch_size):\n",
    "    dataset = CorpusDataset(examples, tokenizer, gcn_vocab_map, MAX_SEQ_LENGTH, config_gcn_embedding_dim)\n",
    "    return DataLoader(dataset=dataset, batch_size=batch_size, shuffle=False, num_workers=4, collate_fn=dataset.pad)\n",
    "\n",
    "\n",
    "train_dataloader = get_pytorch_dataloader(train_examples, tokenizer, batch_size)\n",
    "test_dataloader = get_pytorch_dataloader(test_examples, tokenizer, batch_size)\n",
    "\n",
    "total_train_steps = int(len(train_dataloader) / gradient_accumulation_steps * total_train_epochs)\n",
    "\n",
    "print('Train Classes Count: ', train_classes_num)\n",
    "print('Batch size: ', batch_size)\n",
    "print('Num steps: ', total_train_steps)\n",
    "print('Number of Examples for Training: ', len(train_examples))\n",
    "print('Number of Examples for Training After Dataloader: ', len(train_dataloader) * batch_size)\n",
    "print('Number of Examples for Validate: ', len(test_examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df124ae",
   "metadata": {},
   "source": [
    "#### Step 3.1:   Load Trained VGCN_BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf798ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the pretrain model: VGCN_BERT16_model_pheme_cross_entropy_sw1.pt , epoch: 7 step: -1 valid acc: 0.9517958412098299 weighted avg f1-score_valid: 0.9515764742459526\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VGCN_Bert(\n",
       "  (embeddings): VGCNBertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): BertLayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (vocab_gcn): VocabGraphConvolution(\n",
       "      (fc_hc): Linear(in_features=128, out_features=16, bias=True)\n",
       "      (act_func): ReLU()\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if config_load_model_from_checkpoint and os.path.exists(os.path.join(output_dir, model_file_save)):\n",
    "    checkpoint = torch.load(os.path.join(output_dir, model_file_save), map_location='cpu')\n",
    "    if 'step' in checkpoint:\n",
    "        prev_save_step = checkpoint['step']\n",
    "        start_epoch = checkpoint['epoch']\n",
    "    else:\n",
    "        prev_save_step = -1\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "\n",
    "    valid_acc_prev = checkpoint['valid_acc']\n",
    "    perform_metrics_prev = checkpoint['perform_metrics']\n",
    "    model = VGCN_Bert.from_pretrained(bert_model_scale, state_dict=checkpoint['model_state'], gcn_adj_dim=gcn_vocab_size, \n",
    "        gcn_adj_num=len(gcn_adj_list), gcn_embedding_dim=config_gcn_embedding_dim, num_labels=len(label2idx))\n",
    "\n",
    "    pretrained_dict = checkpoint['model_state']\n",
    "    net_state_dict = model.state_dict()\n",
    "    pretrained_dict_selected = {\n",
    "        k: v for k, v in pretrained_dict.items() if k in net_state_dict}\n",
    "    net_state_dict.update(pretrained_dict_selected)\n",
    "    model.load_state_dict(net_state_dict)\n",
    "\n",
    "    print('Loaded the pretrain model:', model_file_save, ', epoch:', checkpoint['epoch'], 'step:', prev_save_step, 'valid acc:',\n",
    "          checkpoint['valid_acc'], ' '.join(perform_metrics_str)+'_valid:', checkpoint['perform_metrics'])\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1579efee",
   "metadata": {},
   "source": [
    "#### Step 3.2: Evaluate VGCN_BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77ef2042",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, gcn_adj_list,predict_dataloader, batch_size):\n",
    "    # print(\"***** Running prediction *****\")\n",
    "    model.eval()\n",
    "    predict_out = []\n",
    "    all_label_ids = []\n",
    "    ev_loss=0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(predict_dataloader, desc=\"Evaluating\", colour='green'):\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            input_ids, input_mask, segment_ids, y_prob, label_ids, gcn_swop_eye = batch\n",
    "            _, logits = model(gcn_adj_list, gcn_swop_eye,input_ids, segment_ids, input_mask)\n",
    "\n",
    "            if config_loss_criterion=='mse':\n",
    "                if do_softmax_before_mse:\n",
    "                    logits=F.softmax(logits,-1)\n",
    "                loss = F.mse_loss(logits, y_prob)\n",
    "            else:\n",
    "                if loss_weight is None:\n",
    "                    loss = F.cross_entropy(logits.view(-1, num_classes), label_ids)\n",
    "                else:\n",
    "                    loss = F.cross_entropy(logits.view(-1, num_classes), label_ids)\n",
    "                    \n",
    "            ev_loss+=loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(logits, -1)\n",
    "            \n",
    "            predict_out.extend(predicted.tolist())\n",
    "            all_label_ids.extend(label_ids.tolist())\n",
    "            eval_accuracy = predicted.eq(label_ids).sum().item()\n",
    "            total += len(label_ids)\n",
    "            correct += eval_accuracy\n",
    "\n",
    "        f1_metrics=f1_score(np.array(all_label_ids).reshape(-1),\n",
    "            np.array(predict_out).reshape(-1), average='weighted')\n",
    "        print(\"Report:\\n\"+classification_report(np.array(all_label_ids).reshape(-1),\n",
    "            np.array(predict_out).reshape(-1),digits=4))\n",
    "\n",
    "    ev_acc = correct / total\n",
    "    return ev_loss, ev_acc, f1_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1343f224",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|\u001b[32m███████████████████████████████████████\u001b[0m| 199/199 [03:03<00:00,  1.08it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9577    0.9723    0.9650      2167\n",
      "           1     0.9384    0.9076    0.9228      1007\n",
      "\n",
      "    accuracy                         0.9518      3174\n",
      "   macro avg     0.9481    0.9400    0.9439      3174\n",
      "weighted avg     0.9516    0.9518    0.9516      3174\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(43.562897340161726, 0.9517958412098299, 0.9515764742459526)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model, gcn_adj_list, test_dataloader, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0086eb13",
   "metadata": {},
   "source": [
    "#### Step 4.1: Get VGCN_BERT Model Pooled Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af635dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pooled_out(model, gcn_adj_list, predict_dataloader):\n",
    "    \n",
    "    outputs = None\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(predict_dataloader, desc=\"Evaluating\", colour='green'):\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            input_ids, input_mask, segment_ids, y_prob, label_ids, gcn_swop_eye = batch\n",
    "            pooled_output, _= model(gcn_adj_list, gcn_swop_eye, input_ids, segment_ids, input_mask)\n",
    "            \n",
    "            if outputs is None:\n",
    "                outputs = pooled_output.detach().cpu().numpy()\n",
    "            else:\n",
    "                outputs = np.append(outputs, pooled_output.detach().cpu().numpy(), axis=0)\n",
    "    \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c0a1cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|\u001b[32m███████████████████████████████████████\u001b[0m| 482/482 [19:42<00:00,  2.45s/it]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "train_pooled_outputs = get_pooled_out(model, gcn_adj_list, train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4828bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|\u001b[32m███████████████████████████████████████\u001b[0m| 199/199 [08:25<00:00,  2.54s/it]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "test_pooled_outputs = get_pooled_out(model, gcn_adj_list, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c95db355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7709, 768) (3174, 768)\n"
     ]
    }
   ],
   "source": [
    "print(train_pooled_outputs.shape, test_pooled_outputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c461f45",
   "metadata": {},
   "source": [
    "#### Step 4.2: Reorganize Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30f1c274",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index_list = []\n",
    "with open('./data/PHEME-SEG/train_index_list.txt','r') as file:\n",
    "    lines = file.readlines()\n",
    "    for each_line in lines:\n",
    "        train_index_list.append(int(each_line))\n",
    "        \n",
    "test_index_list = []\n",
    "with open('./data/PHEME-SEG/test_index_list.txt','r') as file:\n",
    "    lines = file.readlines()\n",
    "    for each_line in lines:\n",
    "        test_index_list.append(int(each_line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f6c4842",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = []\n",
    "with open('./data/PHEME-SEG/train_label.txt','r') as file:\n",
    "    lines = file.readlines()\n",
    "    for each_line in lines:\n",
    "        train_label.append(int(each_line))\n",
    "\n",
    "test_label = []\n",
    "with open('./data/PHEME-SEG/test_label.txt','r') as file:\n",
    "    lines = file.readlines()\n",
    "    for each_line in lines:\n",
    "        test_label.append(int(each_line))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e5549fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emb</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[-0.86166054, -0.8739809, -0.85248774, 0.7952...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[0.96591175, 0.9302482, 0.99016285, -0.77246,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[0.44478518, 0.7063558, 0.68172604, -0.073655...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[-0.70401967, -0.8766344, -0.8538985, 0.69324...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[-0.9065526, -0.8701292, -0.75249386, 0.66197...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[[-0.71836257, -0.87511003, -0.83389866, 0.539...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[[-0.50658464, -0.80477905, -0.86885136, 0.411...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[[-0.6149378, -0.87986404, -0.36435467, 0.3291...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[[0.39291537, 0.75704014, 0.31513816, 0.209349...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[[-0.5231502, -0.81665456, -0.71154517, 0.4023...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 emb  label\n",
       "0  [[-0.86166054, -0.8739809, -0.85248774, 0.7952...      0\n",
       "1  [[0.96591175, 0.9302482, 0.99016285, -0.77246,...      1\n",
       "2  [[0.44478518, 0.7063558, 0.68172604, -0.073655...      1\n",
       "3  [[-0.70401967, -0.8766344, -0.8538985, 0.69324...      0\n",
       "4  [[-0.9065526, -0.8701292, -0.75249386, 0.66197...      0\n",
       "5  [[-0.71836257, -0.87511003, -0.83389866, 0.539...      0\n",
       "6  [[-0.50658464, -0.80477905, -0.86885136, 0.411...      0\n",
       "7  [[-0.6149378, -0.87986404, -0.36435467, 0.3291...      0\n",
       "8  [[0.39291537, 0.75704014, 0.31513816, 0.209349...      1\n",
       "9  [[-0.5231502, -0.81665456, -0.71154517, 0.4023...      0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x = {}\n",
    "\n",
    "for l, emb in zip(train_index_list,train_pooled_outputs):\n",
    "    if l in train_x.keys():\n",
    "        # np.vstack on lists represents features concatenation \n",
    "        train_x[l]  =np.vstack([train_x[l], emb])\n",
    "    else:\n",
    "        train_x[l] = [emb]\n",
    "\n",
    "train_l_final = []\n",
    "label_l_final = []\n",
    "\n",
    "for k in train_x.keys():\n",
    "    train_l_final.append(train_x[k])\n",
    "    label_l_final.append(train_label[k])\n",
    "\n",
    "df_train = pd.DataFrame({'emb': train_l_final, 'label': label_l_final})\n",
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9292e1d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emb</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[0.8434905, 0.52015185, -0.8269284, -0.496207...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[0.27310348, -0.2630541, 0.8885846, -0.721813...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[-0.63160765, -0.7697314, 0.47586825, -0.0687...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[-0.88583523, -0.85898525, -0.5769335, 0.6244...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[-0.8513122, -0.8424746, -0.43319342, 0.48309...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[[-0.8911206, -0.8823243, -0.6351123, 0.646510...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[[0.13779208, 0.4994552, 0.045249026, -0.05155...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[[-0.9019906, -0.87892944, 0.0929451, 0.581533...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[[-0.7931954, -0.7404746, -0.54706, 0.42936206...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[[-0.85148966, -0.86220515, -0.20071036, 0.643...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 emb  label\n",
       "0  [[0.8434905, 0.52015185, -0.8269284, -0.496207...      1\n",
       "1  [[0.27310348, -0.2630541, 0.8885846, -0.721813...      0\n",
       "2  [[-0.63160765, -0.7697314, 0.47586825, -0.0687...      0\n",
       "3  [[-0.88583523, -0.85898525, -0.5769335, 0.6244...      0\n",
       "4  [[-0.8513122, -0.8424746, -0.43319342, 0.48309...      0\n",
       "5  [[-0.8911206, -0.8823243, -0.6351123, 0.646510...      0\n",
       "6  [[0.13779208, 0.4994552, 0.045249026, -0.05155...      1\n",
       "7  [[-0.9019906, -0.87892944, 0.0929451, 0.581533...      0\n",
       "8  [[-0.7931954, -0.7404746, -0.54706, 0.42936206...      0\n",
       "9  [[-0.85148966, -0.86220515, -0.20071036, 0.643...      0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x = {}\n",
    "\n",
    "for l, emb in zip(test_index_list,test_pooled_outputs):\n",
    "    if l in test_x.keys():\n",
    "        # np.vstack on lists represents features concatenation \n",
    "        test_x[l]  =np.vstack([test_x[l], emb])\n",
    "    else:\n",
    "        test_x[l] = [emb]\n",
    "\n",
    "test_l_final = []\n",
    "tlabel_l_final = []\n",
    "for k in test_x.keys():\n",
    "    test_l_final.append(test_x[k])\n",
    "    tlabel_l_final.append(test_label[k])\n",
    "\n",
    "df_test = pd.DataFrame({'emb': test_l_final, 'label': tlabel_l_final})\n",
    "df_test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e8a3d22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_val, df_test = train_test_split(df_test, test_size=0.5, random_state=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "811ef558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5221, 2) (1074, 2) (1074, 2)\n"
     ]
    }
   ],
   "source": [
    "print(df_train.shape,df_val.shape,df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138c04c4",
   "metadata": {},
   "source": [
    "#### Step 4.3: Generate Data Generator for Joint Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "03cf37b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_dict = {\n",
    "    \"text_comments\": [[7,663], [3, 232], [5, 93]],\n",
    "    \"text_comments_refined\": [[23, 227], [6, 179], [6, 179]],\n",
    "}\n",
    "\n",
    "batch = batch_dict[\"text_comments_refined\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "effcfc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator(df, batch_size, batches_per_epoch):\n",
    "    num_sequences = len(df['emb'].to_list())\n",
    "    assert batch_size * batches_per_epoch == num_sequences\n",
    "    num_features= 768\n",
    "\n",
    "    x_list= df['emb'].to_list()\n",
    "    y_list =  df.label.to_list()\n",
    "    # Generate batches\n",
    "    while True:\n",
    "        for b in range(batches_per_epoch):\n",
    "            longest_index = (b + 1) * batch_size - 1\n",
    "            timesteps = len(max(df['emb'].to_list()[:(b + 1) * batch_size][-batch_size:], key=len))\n",
    "            x_train = np.full((batch_size, timesteps, num_features), -99.)\n",
    "            y_train = np.zeros((batch_size,  1))\n",
    "            for i in range(batch_size):\n",
    "                li = b * batch_size + i\n",
    "                x_train[i, 0:len(x_list[li]), :] = x_list[li]\n",
    "                y_train[i] = y_list[li]\n",
    "            yield x_train, y_train\n",
    "            \n",
    "def val_generator(df,batch_size_val,batches_per_epoch_val):\n",
    "    num_sequences_val = len(df['emb'].to_list())\n",
    "    assert batch_size_val * batches_per_epoch_val == num_sequences_val\n",
    "    num_features= 768\n",
    "\n",
    "    x_list= df['emb'].to_list()\n",
    "    y_list =  df.label.to_list()\n",
    "    # Generate batches\n",
    "    while True:\n",
    "        for b in range(batches_per_epoch_val):\n",
    "            longest_index = (b + 1) * batch_size_val - 1\n",
    "            timesteps = len(max(df['emb'].to_list()[:(b + 1) * batch_size_val][-31:], key=len))\n",
    "            x_val = np.full((batch_size_val, timesteps, num_features), -99.)\n",
    "            y_val = np.zeros((batch_size_val,  1))\n",
    "            for i in range(batch_size_val):\n",
    "                li = b * batch_size_val + i\n",
    "                x_val[i, 0:len(x_list[li]), :] = x_list[li]\n",
    "                y_val[i] = y_list[li]\n",
    "            yield x_val, y_val\n",
    "            \n",
    "def test_generator(df,batch_size_test, batches_per_epoch_test):\n",
    "    num_sequences_test = len(df['emb'].to_list())\n",
    "    assert batch_size_test * batches_per_epoch_test == num_sequences_test\n",
    "    num_features= 768\n",
    "\n",
    "    x_list= df['emb'].to_list()\n",
    "    y_list =  df.label.to_list()\n",
    "    # Generate batches\n",
    "    while True:\n",
    "        for b in range(batches_per_epoch_test):\n",
    "            longest_index = (b + 1) * batch_size_test - 1\n",
    "            timesteps = len(max(df['emb'].to_list()[:(b + 1) * batch_size_test][-31:], key=len))\n",
    "            # print(len(df_train['emb'].to_list()[:b+batch_size][-7:]))\n",
    "            x_test = np.full((batch_size_test, timesteps, num_features), -99.)\n",
    "            y_test = np.zeros((batch_size_test,  1))\n",
    "            for i in range(batch_size_test):\n",
    "                li = b * batch_size_test + i\n",
    "                x_test[i, 0:len(x_list[li]), :] = x_list[li]\n",
    "                y_test[i] = y_list[li]\n",
    "            yield x_test, y_test            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "642aa512",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_generator(df_train, batch[0][0], batch[0][1])\n",
    "val_data = val_generator(df_val, batch[1][0], batch[1][1])\n",
    "test_data = test_generator(df_test, batch[2][0], batch[2][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "af86ee49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def cul_all_metrics(y_true, y_pred, pos_label=1):\n",
    "    return {\"accuracy\": float(\"%.5f\" % accuracy_score(y_true=y_true, y_pred=y_pred)),\n",
    "            \"precision\": float(\"%.5f\" % precision_score(y_true=y_true, y_pred=y_pred, pos_label=pos_label, average=\"weighted\")),\n",
    "            \"recall\": float(\"%.5f\" % recall_score(y_true=y_true, y_pred=y_pred, pos_label=pos_label, average=\"weighted\")),\n",
    "            \"f1-score\": float(\"%.5f\" % f1_score(y_true=y_true, y_pred=y_pred, average=\"weighted\")),\n",
    "           }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081b520a",
   "metadata": {},
   "source": [
    "#### Step 5: Build \"Recurrence over VGCN_BERT\" Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "716ee802",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-21 05:50:33.959563: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2022-03-21 05:50:34.029278: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2099965000 Hz\n",
      "2022-03-21 05:50:34.034231: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x562cf6f0ae80 executing computations on platform Host. Devices:\n",
      "2022-03-21 05:50:34.034367: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "features (InputLayer)        [(None, None, 768)]       0         \n",
      "_________________________________________________________________\n",
      "masking (Masking)            (None, None, 768)         0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 100)               347600    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 30)                3030      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 62        \n",
      "=================================================================\n",
      "Total params: 350,692\n",
      "Trainable params: 350,692\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import h5py\n",
    "\n",
    "text_input = keras.Input(shape=(None,768,), dtype='float32', name='features')\n",
    "l_mask = keras.layers.Masking(mask_value=-99.)(text_input) \n",
    "encoded_text = keras.layers.LSTM(100,)(l_mask)\n",
    "out_dense = keras.layers.Dense(30, activation='relu')(encoded_text)\n",
    "out = keras.layers.Dense(2, activation='softmax')(out_dense)\n",
    "R_Model = keras.Model(text_input, out)\n",
    "R_Model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
    "R_Model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4987216e",
   "metadata": {},
   "outputs": [],
   "source": [
    "call_reduce = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_acc', factor=0.95, patience=3, verbose=2,\n",
    "                                mode='auto', min_delta=0.01, cooldown=0, min_lr=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "84358d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 227 steps, validate for 179 steps\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-21 05:50:55.937086: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_cudnn_lstm_with_fallback_5844_7301' and '__inference___backward_standard_lstm_7406_8003_specialized_for_StatefulPartitionedCall_at___inference_distributed_function_8125' both implement 'lstm_cef586cc-be0f-4051-9559-581e29cdf9cb' but their signatures do not match.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "226/227 [============================>.] - ETA: 0s - loss: 0.0255 - acc: 0.9942"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-21 05:51:11.729468: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference_cudnn_lstm_with_fallback_9377' and '__inference_standard_lstm_9036_specialized_for_model_lstm_StatefulPartitionedCall_at___inference_distributed_function_10907' both implement 'lstm_f4989e69-95b6-4a62-b8a0-cd5e51be0151' but their signatures do not match.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "227/227 [==============================] - 35s 153ms/step - loss: 0.0254 - acc: 0.9943 - val_loss: 0.2022 - val_acc: 0.9525\n",
      "Epoch 2/10\n",
      "227/227 [==============================] - 9s 39ms/step - loss: 0.0164 - acc: 0.9962 - val_loss: 0.2107 - val_acc: 0.9562\n",
      "Epoch 3/10\n",
      "227/227 [==============================] - 14s 63ms/step - loss: 0.0147 - acc: 0.9973 - val_loss: 0.2585 - val_acc: 0.9562\n",
      "Epoch 4/10\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.0156 - acc: 0.9967\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0009500000451225787.\n",
      "227/227 [==============================] - 20s 88ms/step - loss: 0.0155 - acc: 0.9967 - val_loss: 0.2488 - val_acc: 0.9562\n",
      "Epoch 5/10\n",
      "227/227 [==============================] - 14s 60ms/step - loss: 0.0134 - acc: 0.9973 - val_loss: 0.2939 - val_acc: 0.9525\n",
      "Epoch 6/10\n",
      "227/227 [==============================] - 8s 35ms/step - loss: 0.0125 - acc: 0.9977 - val_loss: 0.2945 - val_acc: 0.9544\n",
      "Epoch 7/10\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.0124 - acc: 0.9977\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0009025000152178108.\n",
      "227/227 [==============================] - 8s 34ms/step - loss: 0.0124 - acc: 0.9977 - val_loss: 0.2926 - val_acc: 0.9534\n",
      "Epoch 8/10\n",
      "227/227 [==============================] - 14s 61ms/step - loss: 0.0118 - acc: 0.9977 - val_loss: 0.3157 - val_acc: 0.9544\n",
      "Epoch 9/10\n",
      "227/227 [==============================] - 9s 38ms/step - loss: 0.0116 - acc: 0.9977 - val_loss: 0.3332 - val_acc: 0.9534\n",
      "Epoch 10/10\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.0113 - acc: 0.9979\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0008573750033974647.\n",
      "227/227 [==============================] - 8s 36ms/step - loss: 0.0113 - acc: 0.9979 - val_loss: 0.3426 - val_acc: 0.9525\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1e8ba9fdd0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches_per_epoch = batch[0][1]\n",
    "\n",
    "batches_per_epoch_val= batch[1][1]\n",
    "\n",
    "R_Model.fit(train_data, steps_per_epoch=batches_per_epoch, epochs=10,\n",
    "                    validation_data=val_data, validation_steps=batches_per_epoch_val, callbacks =[call_reduce] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7edaee8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_per_epoch_test = batch[2][1]\n",
    "\n",
    "test_data = test_generator(df_test, batch[2][0], batch[2][1])\n",
    "r_score = R_Model.predict_generator(test_data, steps=batches_per_epoch_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fb268b59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.96089,\n",
       " 'precision': 0.96082,\n",
       " 'recall': 0.96089,\n",
       " 'f1-score': 0.96084}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_pred = np.argmax(r_score, axis=1).tolist()\n",
    "label = df_test.label.to_list()\n",
    "\n",
    "cul_all_metrics(label, r_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc669746",
   "metadata": {},
   "source": [
    "#### Step 6: Build \"Transformer over VGCN_BERT\" Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dbd7cfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads=8):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        assert (\n",
    "            embed_dim % num_heads == 0\n",
    "        ), \"embedding dimension not divisible by num heads\"\n",
    "        self.projection_dim = embed_dim // num_heads\n",
    "        self.wq = keras.layers.Dense(embed_dim)\n",
    "        self.wk = keras.layers.Dense(embed_dim)\n",
    "        self.wv = keras.layers.Dense(embed_dim)\n",
    "        self.combine_heads = keras.layers.Dense(embed_dim)\n",
    "\n",
    "    def attention(self, q, k, v):\n",
    "        score = tf.matmul(q, k, transpose_b=True)\n",
    "        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "        scaled_score = score / tf.math.sqrt(dk)\n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
    "        output = tf.matmul(weights, v)\n",
    "        return output, weights\n",
    "\n",
    "    def separate_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, x):\n",
    "        # x.shape = [batch_size, seq_len, embedding_dim]\n",
    "        batch_size = tf.shape(x)[0]\n",
    "        q = self.wq(x)  # (batch_size, seq_len, embed_dim)\n",
    "        k = self.wk(x)  # (batch_size, seq_len, embed_dim)\n",
    "        v = self.wv(x)  # (batch_size, seq_len, embed_dim)\n",
    "        q = self.separate_heads(\n",
    "            q, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        k = self.separate_heads(\n",
    "            k, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        v = self.separate_heads(\n",
    "            v, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        attention, weights = self.attention(q, k, v)\n",
    "        attention = tf.transpose(\n",
    "            attention, perm=[0, 2, 1, 3]\n",
    "        )  # (batch_size, seq_len, num_heads, projection_dim)\n",
    "        concat_attention = tf.reshape(\n",
    "            attention, (batch_size, -1, self.embed_dim)\n",
    "        )  # (batch_size, seq_len, embed_dim)\n",
    "        output = self.combine_heads(\n",
    "            concat_attention\n",
    "        )  # (batch_size, seq_len, embed_dim)\n",
    "        return output\n",
    "    \n",
    "class TransformerLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerLayer, self).__init__()\n",
    "\n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [\n",
    "                keras.layers.Dense(ff_dim, activation=\"relu\"),\n",
    "                keras.layers.Dense(embed_dim),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training):\n",
    "        attn_output = self.att(x)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e5dfb2da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "features (InputLayer)        [(None, None, 768)]       0         \n",
      "_________________________________________________________________\n",
      "masking_1 (Masking)          (None, None, 768)         0         \n",
      "_________________________________________________________________\n",
      "transformer_layer (Transform (None, None, 768)         2415392   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               347600    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 30)                3030      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 2)                 62        \n",
      "=================================================================\n",
      "Total params: 2,766,084\n",
      "Trainable params: 2,766,084\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "text_input = keras.Input(shape=(None,768,), dtype='float32', name='features')\n",
    "l_mask = keras.layers.Masking(mask_value=-99.)(text_input) \n",
    "transformer_encodings = TransformerLayer(embed_dim=768, num_heads=1, ff_dim=32)(l_mask)\n",
    "encoded_texts = keras.layers.LSTM(100,)(transformer_encodings)\n",
    "out_dense = keras.layers.Dense(30, activation='relu')(encoded_texts)\n",
    "out = keras.layers.Dense(2, activation='softmax')(out_dense)\n",
    "T_Model = keras.Model(text_input, out)\n",
    "T_Model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "T_Model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "21e1681b",
   "metadata": {},
   "outputs": [],
   "source": [
    "call_reduce = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_acc', factor=0.95, patience=3, verbose=2,\n",
    "                                mode='auto', min_delta=0.01, cooldown=0, min_lr=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "35496c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 227 steps, validate for 179 steps\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-21 05:57:17.347064: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_cudnn_lstm_with_fallback_94597_94779' and '__inference___backward_standard_lstm_94884_95367_specialized_for_StatefulPartitionedCall_at___inference_distributed_function_95945' both implement 'lstm_61a9ed54-e546-4f25-99d6-a2fcdd7258b6' but their signatures do not match.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "226/227 [============================>.] - ETA: 0s - loss: 0.0265 - acc: 0.9948"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-21 05:57:36.771864: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference_standard_lstm_97090' and '__inference_standard_lstm_97090_specialized_for_model_1_lstm_1_StatefulPartitionedCall_at___inference_distributed_function_97456' both implement 'lstm_c26cc4b2-886f-4f4d-9e54-37960ba4941c' but their signatures do not match.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "227/227 [==============================] - 37s 162ms/step - loss: 0.0264 - acc: 0.9948 - val_loss: 0.2370 - val_acc: 0.9479\n",
      "Epoch 2/10\n",
      "227/227 [==============================] - 17s 77ms/step - loss: 0.0225 - acc: 0.9958 - val_loss: 0.2344 - val_acc: 0.9516\n",
      "Epoch 3/10\n",
      "227/227 [==============================] - 20s 86ms/step - loss: 0.0225 - acc: 0.9950 - val_loss: 0.2386 - val_acc: 0.9525\n",
      "Epoch 4/10\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.0175 - acc: 0.9965\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0009500000451225787.\n",
      "227/227 [==============================] - 18s 79ms/step - loss: 0.0174 - acc: 0.9966 - val_loss: 0.2873 - val_acc: 0.9525\n",
      "Epoch 5/10\n",
      "227/227 [==============================] - 18s 79ms/step - loss: 0.0196 - acc: 0.9958 - val_loss: 0.2694 - val_acc: 0.9507\n",
      "Epoch 6/10\n",
      "227/227 [==============================] - 29s 127ms/step - loss: 0.0191 - acc: 0.9964 - val_loss: 0.2784 - val_acc: 0.9525\n",
      "Epoch 7/10\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.0178 - acc: 0.9967\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0009025000152178108.\n",
      "227/227 [==============================] - 19s 83ms/step - loss: 0.0177 - acc: 0.9967 - val_loss: 0.2985 - val_acc: 0.9507\n",
      "Epoch 8/10\n",
      "227/227 [==============================] - 28s 122ms/step - loss: 0.0180 - acc: 0.9960 - val_loss: 0.3157 - val_acc: 0.9516\n",
      "Epoch 9/10\n",
      "227/227 [==============================] - 22s 97ms/step - loss: 0.0174 - acc: 0.9964 - val_loss: 0.3111 - val_acc: 0.9479\n",
      "Epoch 10/10\n",
      "226/227 [============================>.] - ETA: 0s - loss: 0.0171 - acc: 0.9962\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0008573750033974647.\n",
      "227/227 [==============================] - 18s 81ms/step - loss: 0.0171 - acc: 0.9962 - val_loss: 0.3126 - val_acc: 0.9497\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1d241496d0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches_per_epoch = batch[0][1]\n",
    "\n",
    "batches_per_epoch_val= batch[1][1]\n",
    "\n",
    "T_Model.fit(train_data, steps_per_epoch=batches_per_epoch, epochs=10,\n",
    "                    validation_data=val_data, validation_steps=batches_per_epoch_val, callbacks =[call_reduce] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ca5768ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_per_epoch_test = batch[2][1]\n",
    "\n",
    "test_data = test_generator(df_test, batch[2][0], batch[2][1])\n",
    "t_score = T_Model.predict_generator(test_data, steps=batches_per_epoch_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d6ef4707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.96276,\n",
       " 'precision': 0.96281,\n",
       " 'recall': 0.96276,\n",
       " 'f1-score': 0.96278}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_pred = np.argmax(t_score, axis=1).tolist()\n",
    "label = df_test.label.to_list()\n",
    "\n",
    "cul_all_metrics(label, t_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e026654",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "409.6px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
