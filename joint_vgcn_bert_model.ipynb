{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbfba28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import argparse\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from pytorch_pretrained_bert.modeling import BertModel, BertConfig, WEIGHTS_NAME, CONFIG_NAME\n",
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "from pytorch_pretrained_bert.optimization import BertAdam  # , warmup_linear\n",
    "\n",
    "\n",
    "from utils import *\n",
    "from model_vgcn_bert import VGCN_Bert\n",
    "from model_vgcn_bert_ablation import VGCN_Bert_Ablation\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54f7655",
   "metadata": {},
   "source": [
    "#### Step 1:   Configurations for Evaluating VGCN_BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6ad4e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------STEP 1: CONFIGURATIONS FOR VGCN_BERT MODEL--------\n",
      "Dataset:  pheme\n",
      "Will Load Model from Checkpoint:  True\n",
      "Will Do Ablation Tests:  True\n",
      "Ablation Choice:  vgcn_only\n",
      "Will Delete Stop Words:  True\n",
      "Vocab GCN Hidden Dim: vocab_size -> 128 -> 1\n",
      "Learning Rate0:  1e-05\n",
      "Weight Decay:  0.01\n",
      "Loss Criterion:  cross_entropy\n",
      "Will Perform Softmax before MSE:  True\n",
      "Vocab Adjcent:  all\n",
      "MAX_SEQ_LENGTH:  250\n",
      "Perform Metrics:  ['weighted avg', 'f1-score']\n",
      "Load Model File Name:  Model_VGCN_BERT_Ablation_pheme_text_comments.pt\n"
     ]
    }
   ],
   "source": [
    "args = {\"ds\": \"pheme\", \"load\": 1, \"sw\": 1,\n",
    "        \"lr\": 1e-5, \"l2\": 0.01}\n",
    "\n",
    "config_dataset = args[\"ds\"]\n",
    "config_load_model_from_checkpoint = True if args[\"load\"] == 1 else False\n",
    "config_use_stopwords = True if args[\"sw\"] == 1 else False\n",
    "config_learning_rate0 = args[\"lr\"]\n",
    "config_l2_decay = args[\"l2\"]\n",
    "\n",
    "config_will_do_ablation_test = True\n",
    "config_ablation_choice = \"vgcn_only\" # no_attention / vgcn_only / bert_only\n",
    "config_model_type = 'VGCN_BERT' if not config_will_do_ablation_test else 'VGCN_BERT_Ablation'\n",
    "config_data_type = \"text_comments\" # text_comments / text_only / comments_only\n",
    "\n",
    "\n",
    "config_gcn_embedding_dim = 16 if not config_will_do_ablation_test else 1\n",
    "config_warmup_proportion = 0.1\n",
    "config_vocab_adj = 'all'  # pmi / tf / all\n",
    "config_adj_npmi_threshold = 0.2\n",
    "config_adj_tf_threshold = 0\n",
    "config_loss_criterion = 'cross_entropy'\n",
    "config_output_num_features = 768\n",
    "MAX_SEQ_LENGTH = 200 + config_gcn_embedding_dim if not config_will_do_ablation_test else 250\n",
    "\n",
    "if config_will_do_ablation_test and config_ablation_choice == \"no_attention\":\n",
    "    config_output_num_features = 768 * 2\n",
    "\n",
    "total_train_epochs = 9\n",
    "batch_size = 16  # 12\n",
    "gradient_accumulation_steps = 1\n",
    "if config_dataset == 'pheme':\n",
    "    bert_model_scale = 'bert-base-uncased'\n",
    "elif config_dataset == 'weibo':\n",
    "    bert_model_scale = 'bert-base-chinese'\n",
    "\n",
    "do_lower_case = True\n",
    "perform_metrics_str = ['weighted avg', 'f1-score']\n",
    "do_softmax_before_mse = True\n",
    "\n",
    "data_dir = './prepared_data/' + config_dataset + '_' + config_data_type\n",
    "output_dir = './model_output/' if not config_will_do_ablation_test else './model_output/ablation_tests/'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "\n",
    "\n",
    "model_load_file = 'Model_' + config_model_type + '_' \\\n",
    "                + config_dataset + '_' + config_data_type + '.pt'\n",
    "\n",
    "print('----------STEP 1: CONFIGURATIONS FOR VGCN_BERT MODEL--------')\n",
    "print('Dataset: ', config_dataset)\n",
    "print('Will Load Model from Checkpoint: ', config_load_model_from_checkpoint)\n",
    "print('Will Do Ablation Tests: ', config_will_do_ablation_test)\n",
    "if config_will_do_ablation_test:\n",
    "    print('Ablation Choice: ', config_ablation_choice)\n",
    "print('Will Delete Stop Words: ', config_use_stopwords)\n",
    "print('Vocab GCN Hidden Dim: vocab_size -> 128 -> ' + str(config_gcn_embedding_dim))\n",
    "print('Learning Rate0: ', config_learning_rate0)\n",
    "print('Weight Decay: ', config_l2_decay)\n",
    "print('Loss Criterion: ', config_loss_criterion)\n",
    "print('Will Perform Softmax before MSE: ', do_softmax_before_mse)\n",
    "print('Vocab Adjcent: ', config_vocab_adj)\n",
    "print('MAX_SEQ_LENGTH: ', MAX_SEQ_LENGTH)\n",
    "print('Perform Metrics: ', perform_metrics_str)\n",
    "print('Load Model File Name: ', model_load_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79eede6",
   "metadata": {},
   "source": [
    "#### Step 2.1: Prepare Dataset & Load Vocabulary Adjacent Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e0e7e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------STEP 2: PREPARE DATASET & LOAD VOCABULARY ADJACENT MATRIX----------\n",
      " Load and seperate pheme dataset, with vocabulary graph adjacent matrix\n"
     ]
    }
   ],
   "source": [
    "print('----------STEP 2: PREPARE DATASET & LOAD VOCABULARY ADJACENT MATRIX----------')\n",
    "print(' Load and seperate', config_dataset, 'dataset, with vocabulary graph adjacent matrix')\n",
    "\n",
    "objects = []\n",
    "names = ['index_label', 'train_label', 'train_label_prob', 'test_label',\n",
    "         'test_label_prob', 'clean_docs', 'vocab_adj_tf', 'vocab_adj_pmi', 'vocab_map']\n",
    "\n",
    "for i in range(len(names)):\n",
    "    datafile = data_dir + \"/data_%s.%s\" % (config_dataset, names[i])\n",
    "    with open(datafile, 'rb') as f:\n",
    "        objects.append(pkl.load(f, encoding='latin1'))\n",
    "\n",
    "index_labels_list, train_label, train_label_prob, test_label, test_label_prob, shuffled_clean_docs, gcn_vocab_adj_tf, gcn_vocab_adj_pmi, gcn_vocab_map = tuple(objects)\n",
    "\n",
    "label2idx = index_labels_list[0]\n",
    "idx2label = index_labels_list[1]\n",
    "\n",
    "all_labels = np.hstack((train_label, test_label))\n",
    "all_labels_prob = np.vstack((train_label_prob, test_label_prob))\n",
    "\n",
    "examples = []\n",
    "for i, text in enumerate(shuffled_clean_docs):\n",
    "    example = InputExample(i, text.strip(), confidence=all_labels_prob[i], label=all_labels[i])\n",
    "    examples.append(example)\n",
    "\n",
    "num_classes = len(label2idx)\n",
    "gcn_vocab_size = len(gcn_vocab_map)\n",
    "train_size = len(train_label)\n",
    "test_size = len(test_label)\n",
    "\n",
    "indexs = np.arange(0, len(examples))\n",
    "train_examples = [examples[i] for i in indexs[:train_size]]\n",
    "test_examples = [examples[i] for i in indexs[train_size:train_size + test_size]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa16c2b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero ratio for vocab adj 0th: 79.40479542\n",
      "Zero ratio for vocab adj 1th: 95.22829647\n"
     ]
    }
   ],
   "source": [
    "if config_adj_tf_threshold > 0:\n",
    "    gcn_vocab_adj_tf.data *= (gcn_vocab_adj_tf.data > config_adj_tf_threshold)\n",
    "    gcn_vocab_adj_tf.eliminate_zeros()\n",
    "if config_adj_npmi_threshold > 0:\n",
    "    gcn_vocab_adj_pmi.data *= (gcn_vocab_adj_pmi.data > config_adj_npmi_threshold)\n",
    "    gcn_vocab_adj_pmi.eliminate_zeros()\n",
    "\n",
    "if config_vocab_adj == 'pmi':\n",
    "    gcn_vocab_adj_list = [gcn_vocab_adj_pmi]\n",
    "elif config_vocab_adj == 'tf':\n",
    "    gcn_vocab_adj_list = [gcn_vocab_adj_tf]\n",
    "elif config_vocab_adj == 'all':\n",
    "    gcn_vocab_adj_list = [gcn_vocab_adj_tf, gcn_vocab_adj_pmi]\n",
    "\n",
    "norm_gcn_vocab_adj_list = []\n",
    "for i in range(len(gcn_vocab_adj_list)):\n",
    "    adj = gcn_vocab_adj_list[i]\n",
    "\n",
    "    print('Zero ratio for vocab adj %dth: %.8f' %\n",
    "          (i, 100 * (1 - adj.count_nonzero() / (adj.shape[0] * adj.shape[1]))))\n",
    "\n",
    "    adj = normalize_adj(adj)\n",
    "    norm_gcn_vocab_adj_list.append(sparse_scipy2torch(adj.tocoo()).to(device))\n",
    "\n",
    "gcn_adj_list = norm_gcn_vocab_adj_list\n",
    "\n",
    "\n",
    "train_classes_num, train_classes_weight = get_class_count_and_weight(train_label, len(label2idx))\n",
    "loss_weight = torch.tensor(train_classes_weight).to(device)\n",
    "loss_weight = torch.tensor(loss_weight, dtype=torch.float32).to(device)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model_scale, do_lower_case=do_lower_case)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac765aa4",
   "metadata": {},
   "source": [
    "#### Step 2.2:   Prepare PyTorch DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "916e2c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Classes Count:  [5341, 2368]\n",
      "Batch size:  16\n",
      "Num steps:  4338\n",
      "Number of Examples for Training:  7709\n",
      "Number of Examples for Training After Dataloader:  7712\n",
      "Number of Examples for Test:  3174\n"
     ]
    }
   ],
   "source": [
    "def get_pytorch_dataloader(examples, tokenizer, batch_size):\n",
    "    dataset = CorpusDataset(examples, tokenizer, gcn_vocab_map, MAX_SEQ_LENGTH, config_gcn_embedding_dim)\n",
    "    return DataLoader(dataset=dataset, batch_size=batch_size, shuffle=False, num_workers=4, collate_fn=dataset.pad)\n",
    "\n",
    "\n",
    "train_dataloader = get_pytorch_dataloader(train_examples, tokenizer, batch_size)\n",
    "test_dataloader = get_pytorch_dataloader(test_examples, tokenizer, batch_size)\n",
    "\n",
    "total_train_steps = int(len(train_dataloader) / gradient_accumulation_steps * total_train_epochs)\n",
    "\n",
    "print('Train Classes Count: ', train_classes_num)\n",
    "print('Batch size: ', batch_size)\n",
    "print('Num steps: ', total_train_steps)\n",
    "print('Number of Examples for Training: ', len(train_examples))\n",
    "print('Number of Examples for Training After Dataloader: ', len(train_dataloader) * batch_size)\n",
    "print('Number of Examples for Test: ', len(test_examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd22e33e",
   "metadata": {},
   "source": [
    "#### Step 3.1 Ablation Tests (May Skip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "922e13e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the pretrain model: Model_VGCN_BERT_Ablation_pheme_text_comments.pt , epoch: 7 step: -1 test acc: 0.9505356017643353 weighted avg f1-score_test: 0.9504216946195111\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VGCN_Bert_Ablation(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=1536, out_features=2, bias=True)\n",
       "  (vocab_gcn): VocabGraphConvolution(\n",
       "    (fc_hc): Linear(in_features=128, out_features=1, bias=True)\n",
       "    (act_func): ReLU()\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if config_load_model_from_checkpoint and os.path.exists(os.path.join(output_dir, model_load_file)):\n",
    "    checkpoint = torch.load(os.path.join(output_dir, model_load_file), map_location='cpu')\n",
    "    if 'step' in checkpoint:\n",
    "        prev_save_step = checkpoint['step']\n",
    "        start_epoch = checkpoint['epoch']\n",
    "    else:\n",
    "        prev_save_step = -1\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "\n",
    "    test_acc_prev = checkpoint['test_acc']\n",
    "    perform_metrics_prev = checkpoint['perform_metrics']\n",
    "    model = VGCN_Bert_Ablation.from_pretrained(bert_model_scale, state_dict=checkpoint['model_state'], gcn_adj_dim=gcn_vocab_size, \n",
    "        gcn_adj_num=len(gcn_adj_list), gcn_embedding_dim=config_gcn_embedding_dim, num_labels=len(label2idx))\n",
    "\n",
    "    pretrained_dict = checkpoint['model_state']\n",
    "    net_state_dict = model.state_dict()\n",
    "    pretrained_dict_selected = {\n",
    "        k: v for k, v in pretrained_dict.items() if k in net_state_dict}\n",
    "    net_state_dict.update(pretrained_dict_selected)\n",
    "    model.load_state_dict(net_state_dict)\n",
    "\n",
    "    print('Loaded the pretrain model:', model_load_file, ', epoch:', checkpoint['epoch'], 'step:', prev_save_step, 'test acc:',\n",
    "          checkpoint['test_acc'], ' '.join(perform_metrics_str)+'_test:', checkpoint['perform_metrics'])\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d704021a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pooled_out_ablation(model, gcn_adj_list, predict_dataloader, ablation_choice):\n",
    "    \n",
    "    outputs = None\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(predict_dataloader, desc=\"Evaluating\", colour='green'):\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            input_ids, input_mask, segment_ids, y_prob, label_ids, gcn_swop_eye = batch\n",
    "            vgcn_out, pooled_output, cat_out, _= model(gcn_adj_list, gcn_swop_eye, input_ids, segment_ids, input_mask)\n",
    "            \n",
    "            if ablation_choice == 'no_attention':\n",
    "                output = cat_out\n",
    "            elif ablation_choice == 'vgcn_only':\n",
    "                output = vgcn_out\n",
    "            elif ablation_choice == 'bert_only':\n",
    "                output = pooled_output\n",
    "            \n",
    "            if outputs is None:\n",
    "                outputs = output.detach().cpu().numpy()\n",
    "            else:\n",
    "                outputs = np.append(outputs, output.detach().cpu().numpy(), axis=0)\n",
    "    \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "edfb3bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|\u001b[32m████████████████████████████████████████████████████████████████████████████████\u001b[0m| 482/482 [20:46<00:00,  2.59s/it]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "train_pooled_outputs = get_pooled_out_ablation(model, gcn_adj_list, train_dataloader, config_ablation_choice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77c490e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|\u001b[32m████████████████████████████████████████████████████████████████████████████████\u001b[0m| 199/199 [08:37<00:00,  2.60s/it]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "test_pooled_outputs = get_pooled_out_ablation(model, gcn_adj_list, test_dataloader, config_ablation_choice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e19e6a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7709, 768) (3174, 768)\n"
     ]
    }
   ],
   "source": [
    "print(train_pooled_outputs.shape, test_pooled_outputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df124ae",
   "metadata": {},
   "source": [
    "#### Step 3.2:   Load Trained VGCN_BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf798ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if config_load_model_from_checkpoint and os.path.exists(os.path.join(output_dir, model_load_file)):\n",
    "#     checkpoint = torch.load(os.path.join(output_dir, model_load_file), map_location='cpu')\n",
    "#     if 'step' in checkpoint:\n",
    "#         prev_save_step = checkpoint['step']\n",
    "#         start_epoch = checkpoint['epoch']\n",
    "#     else:\n",
    "#         prev_save_step = -1\n",
    "#         start_epoch = checkpoint['epoch'] + 1\n",
    "\n",
    "#     test_acc_prev = checkpoint['test_acc']\n",
    "#     perform_metrics_prev = checkpoint['perform_metrics']\n",
    "#     model = VGCN_Bert.from_pretrained(bert_model_scale, state_dict=checkpoint['model_state'], gcn_adj_dim=gcn_vocab_size, \n",
    "#         gcn_adj_num=len(gcn_adj_list), gcn_embedding_dim=config_gcn_embedding_dim, num_labels=len(label2idx))\n",
    "\n",
    "#     pretrained_dict = checkpoint['model_state']\n",
    "#     net_state_dict = model.state_dict()\n",
    "#     pretrained_dict_selected = {\n",
    "#         k: v for k, v in pretrained_dict.items() if k in net_state_dict}\n",
    "#     net_state_dict.update(pretrained_dict_selected)\n",
    "#     model.load_state_dict(net_state_dict)\n",
    "\n",
    "#     print('Loaded the pretrain model:', model_load_file, ', epoch:', checkpoint['epoch'], 'step:', prev_save_step, 'test acc:',\n",
    "#           checkpoint['test_acc'], ' '.join(perform_metrics_str)+'_test:', checkpoint['perform_metrics'])\n",
    "\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1579efee",
   "metadata": {},
   "source": [
    "#### Step 3.3: Evaluate VGCN_BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77ef2042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate(model, gcn_adj_list,predict_dataloader, batch_size):\n",
    "#     # print(\"***** Running prediction *****\")\n",
    "#     model.eval()\n",
    "#     predict_out = []\n",
    "#     all_label_ids = []\n",
    "#     ev_loss=0\n",
    "#     total = 0\n",
    "#     correct = 0\n",
    "#     with torch.no_grad():\n",
    "#         for batch in tqdm(predict_dataloader, desc=\"Evaluating\", colour='green'):\n",
    "#             batch = tuple(t.to(device) for t in batch)\n",
    "#             input_ids, input_mask, segment_ids, y_prob, label_ids, gcn_swop_eye = batch\n",
    "#             _, logits = model(gcn_adj_list, gcn_swop_eye,input_ids, segment_ids, input_mask)\n",
    "\n",
    "#             if config_loss_criterion=='mse':\n",
    "#                 if do_softmax_before_mse:\n",
    "#                     logits=F.softmax(logits,-1)\n",
    "#                 loss = F.mse_loss(logits, y_prob)\n",
    "#             else:\n",
    "#                 if loss_weight is None:\n",
    "#                     loss = F.cross_entropy(logits.view(-1, num_classes), label_ids)\n",
    "#                 else:\n",
    "#                     loss = F.cross_entropy(logits.view(-1, num_classes), label_ids)\n",
    "                    \n",
    "#             ev_loss+=loss.item()\n",
    "            \n",
    "#             _, predicted = torch.max(logits, -1)\n",
    "            \n",
    "#             predict_out.extend(predicted.tolist())\n",
    "#             all_label_ids.extend(label_ids.tolist())\n",
    "#             eval_accuracy = predicted.eq(label_ids).sum().item()\n",
    "#             total += len(label_ids)\n",
    "#             correct += eval_accuracy\n",
    "\n",
    "#         f1_metrics=f1_score(np.array(all_label_ids).reshape(-1),\n",
    "#             np.array(predict_out).reshape(-1), average='weighted')\n",
    "#         print(\"Report:\\n\"+classification_report(np.array(all_label_ids).reshape(-1),\n",
    "#             np.array(predict_out).reshape(-1),digits=4))\n",
    "\n",
    "#     ev_acc = correct / total\n",
    "#     return ev_loss, ev_acc, f1_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1343f224",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# evaluate(model, gcn_adj_list, test_dataloader, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0086eb13",
   "metadata": {},
   "source": [
    "#### Step 4.1: Get VGCN_BERT Model Pooled Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af635dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_pooled_out(model, gcn_adj_list, predict_dataloader):\n",
    "    \n",
    "#     outputs = None\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         for batch in tqdm(predict_dataloader, desc=\"Evaluating\", colour='green'):\n",
    "#             batch = tuple(t.to(device) for t in batch)\n",
    "#             input_ids, input_mask, segment_ids, y_prob, label_ids, gcn_swop_eye = batch\n",
    "#             pooled_output, _= model(gcn_adj_list, gcn_swop_eye, input_ids, segment_ids, input_mask)\n",
    "            \n",
    "#             if outputs is None:\n",
    "#                 outputs = pooled_output.detach().cpu().numpy()\n",
    "#             else:\n",
    "#                 outputs = np.append(outputs, pooled_output.detach().cpu().numpy(), axis=0)\n",
    "    \n",
    "#     return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c0a1cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_pooled_outputs = get_pooled_out(model, gcn_adj_list, train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4828bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_pooled_outputs = get_pooled_out(model, gcn_adj_list, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c95db355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_pooled_outputs.shape, test_pooled_outputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c461f45",
   "metadata": {},
   "source": [
    "#### Step 4.2: Reorganize Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "30f1c274",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index_path = './data/' + config_dataset.upper() + '-SEG/' + config_data_type + '/train_index_list.txt'\n",
    "test_index_path = './data/' + config_dataset.upper() + '-SEG/' + config_data_type + '/test_index_list.txt'\n",
    "\n",
    "train_index_list = []\n",
    "with open(train_index_path, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "    for each_line in lines:\n",
    "        train_index_list.append(int(each_line))\n",
    "        \n",
    "test_index_list = []\n",
    "with open(test_index_path, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "    for each_line in lines:\n",
    "        test_index_list.append(int(each_line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f6c4842",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label_path = './data/' + config_dataset.upper() + '-SEG/'+ config_data_type +'/train_label.txt'\n",
    "test_label_path = './data/' + config_dataset.upper() + '-SEG/'+ config_data_type +'/test_label.txt'\n",
    "\n",
    "train_label = []\n",
    "with open(train_label_path, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "    for each_line in lines:\n",
    "        train_label.append(int(each_line))\n",
    "\n",
    "test_label = []\n",
    "with open(test_label_path, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "    for each_line in lines:\n",
    "        test_label.append(int(each_line))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9e5549fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emb</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[0.01680187, 0.013852032, 0.01778804, 0.03349...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[0.00043393672, 0.016871663, 0.044724338, 0.0...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[0.04059963, 0.06517336, 0.034778416, 0.05385...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[0.019191245, 0.023653356, 0.02521983, 0.0310...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[0.0085446965, 0.011816889, 0.006936282, 0.01...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[[-0.015042536, -0.011104598, -0.0142199565, -...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[[0.004592642, 0.003820967, 0.00013222545, 0.0...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[[-0.015112469, -0.010178502, -0.008307333, -0...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[[0.012637218, 0.02790481, 0.01790919, 0.03383...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[[-0.009839478, 0.03426922, 0.038110718, 0.075...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 emb  label\n",
       "0  [[0.01680187, 0.013852032, 0.01778804, 0.03349...      0\n",
       "1  [[0.00043393672, 0.016871663, 0.044724338, 0.0...      1\n",
       "2  [[0.04059963, 0.06517336, 0.034778416, 0.05385...      1\n",
       "3  [[0.019191245, 0.023653356, 0.02521983, 0.0310...      0\n",
       "4  [[0.0085446965, 0.011816889, 0.006936282, 0.01...      0\n",
       "5  [[-0.015042536, -0.011104598, -0.0142199565, -...      0\n",
       "6  [[0.004592642, 0.003820967, 0.00013222545, 0.0...      0\n",
       "7  [[-0.015112469, -0.010178502, -0.008307333, -0...      0\n",
       "8  [[0.012637218, 0.02790481, 0.01790919, 0.03383...      1\n",
       "9  [[-0.009839478, 0.03426922, 0.038110718, 0.075...      0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x = {}\n",
    "\n",
    "for l, emb in zip(train_index_list,train_pooled_outputs):\n",
    "    if l in train_x.keys():\n",
    "        # np.vstack on lists represents features concatenation \n",
    "        train_x[l]  =np.vstack([train_x[l], emb])\n",
    "    else:\n",
    "        train_x[l] = [emb]\n",
    "\n",
    "train_l_final = []\n",
    "label_l_final = []\n",
    "\n",
    "for k in train_x.keys():\n",
    "    train_l_final.append(train_x[k])\n",
    "    label_l_final.append(train_label[k])\n",
    "\n",
    "df_train = pd.DataFrame({'emb': train_l_final, 'label': label_l_final})\n",
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9292e1d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emb</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[0.0009316914, 0.03501807, 0.024133163, 0.050...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[0.015521595, 0.022322385, 0.03313172, 0.0583...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[0.01658486, 0.05144207, 0.03161218, 0.062799...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[0.010414546, 0.023666153, 0.0037200954, 0.03...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[0.043209784, 0.0474843, 0.019516794, 0.08808...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[[0.02552834, 0.03534136, 0.00903167, 0.077379...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[[-0.00944491, -0.0048265615, 0.0017341878, 0....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[[0.031745307, 0.051066287, 0.07357788, 0.0641...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[[0.013758717, 0.02846297, 0.023216037, 0.0509...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[[0.002276061, 0.007911013, 0.0038150363, 0.00...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 emb  label\n",
       "0  [[0.0009316914, 0.03501807, 0.024133163, 0.050...      1\n",
       "1  [[0.015521595, 0.022322385, 0.03313172, 0.0583...      0\n",
       "2  [[0.01658486, 0.05144207, 0.03161218, 0.062799...      0\n",
       "3  [[0.010414546, 0.023666153, 0.0037200954, 0.03...      0\n",
       "4  [[0.043209784, 0.0474843, 0.019516794, 0.08808...      0\n",
       "5  [[0.02552834, 0.03534136, 0.00903167, 0.077379...      0\n",
       "6  [[-0.00944491, -0.0048265615, 0.0017341878, 0....      1\n",
       "7  [[0.031745307, 0.051066287, 0.07357788, 0.0641...      0\n",
       "8  [[0.013758717, 0.02846297, 0.023216037, 0.0509...      0\n",
       "9  [[0.002276061, 0.007911013, 0.0038150363, 0.00...      0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x = {}\n",
    "\n",
    "for l, emb in zip(test_index_list,test_pooled_outputs):\n",
    "    if l in test_x.keys():\n",
    "        # np.vstack on lists represents features concatenation \n",
    "        test_x[l]  =np.vstack([test_x[l], emb])\n",
    "    else:\n",
    "        test_x[l] = [emb]\n",
    "\n",
    "test_l_final = []\n",
    "tlabel_l_final = []\n",
    "for k in test_x.keys():\n",
    "    test_l_final.append(test_x[k])\n",
    "    tlabel_l_final.append(test_label[k])\n",
    "\n",
    "df_test = pd.DataFrame({'emb': test_l_final, 'label': tlabel_l_final})\n",
    "df_test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e8a3d22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_val, df_test = train_test_split(df_test, test_size=0.5, random_state=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "811ef558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5221, 2) (1074, 2) (1074, 2)\n"
     ]
    }
   ],
   "source": [
    "print(df_train.shape,df_val.shape,df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138c04c4",
   "metadata": {},
   "source": [
    "#### Step 4.3: Generate Data Generator for Joint Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "03cf37b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_dict = {\n",
    "#     \"text_comments\": [[7,663], [3, 232], [5, 93]],\n",
    "    \"pheme_text_comments_refined\": [[23, 227], [6, 179], [6, 179]],\n",
    "    \"pheme_text_only_refined\": [[23, 227], [3, 271], [3, 271]],\n",
    "    \"pheme_comments_only_refined\": [[23, 227], [2, 341], [1, 683]],\n",
    "    \"weibo_text_comments_refined\": [[3, 1399], [1, 863], [2, 432]],\n",
    "    \"weibo_text_only_refined\": [[3, 1399], [1, 653], [2, 327]],\n",
    "    \"weibo_comments_only_refined\": [[3, 1399], [4, 137], [9, 61]],\n",
    "}\n",
    "\n",
    "batch = batch_dict[config_dataset + '_' + config_data_type + '_refined']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "effcfc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator(df, batch_size, batches_per_epoch):\n",
    "    num_sequences = len(df['emb'].to_list())\n",
    "    assert batch_size * batches_per_epoch == num_sequences\n",
    "    num_features= config_output_num_features\n",
    "\n",
    "    x_list= df['emb'].to_list()\n",
    "    y_list =  df.label.to_list()\n",
    "    # Generate batches\n",
    "    while True:\n",
    "        for b in range(batches_per_epoch):\n",
    "            longest_index = (b + 1) * batch_size - 1\n",
    "            timesteps = len(max(df['emb'].to_list()[:(b + 1) * batch_size][-batch_size:], key=len))\n",
    "            x_train = np.full((batch_size, timesteps, num_features), -99.)\n",
    "            y_train = np.zeros((batch_size,  1))\n",
    "            for i in range(batch_size):\n",
    "                li = b * batch_size + i\n",
    "                x_train[i, 0:len(x_list[li]), :] = x_list[li]\n",
    "                y_train[i] = y_list[li]\n",
    "            yield x_train, y_train\n",
    "            \n",
    "def val_generator(df,batch_size_val,batches_per_epoch_val):\n",
    "    num_sequences_val = len(df['emb'].to_list())\n",
    "    assert batch_size_val * batches_per_epoch_val == num_sequences_val\n",
    "    num_features= config_output_num_features\n",
    "\n",
    "    x_list= df['emb'].to_list()\n",
    "    y_list =  df.label.to_list()\n",
    "    # Generate batches\n",
    "    while True:\n",
    "        for b in range(batches_per_epoch_val):\n",
    "            longest_index = (b + 1) * batch_size_val - 1\n",
    "            timesteps = len(max(df['emb'].to_list()[:(b + 1) * batch_size_val][-31:], key=len))\n",
    "            x_val = np.full((batch_size_val, timesteps, num_features), -99.)\n",
    "            y_val = np.zeros((batch_size_val,  1))\n",
    "            for i in range(batch_size_val):\n",
    "                li = b * batch_size_val + i\n",
    "                x_val[i, 0:len(x_list[li]), :] = x_list[li]\n",
    "                y_val[i] = y_list[li]\n",
    "            yield x_val, y_val\n",
    "            \n",
    "def test_generator(df,batch_size_test, batches_per_epoch_test):\n",
    "    num_sequences_test = len(df['emb'].to_list())\n",
    "    assert batch_size_test * batches_per_epoch_test == num_sequences_test\n",
    "    num_features= config_output_num_features\n",
    "\n",
    "    x_list= df['emb'].to_list()\n",
    "    y_list =  df.label.to_list()\n",
    "    # Generate batches\n",
    "    while True:\n",
    "        for b in range(batches_per_epoch_test):\n",
    "            longest_index = (b + 1) * batch_size_test - 1\n",
    "            timesteps = len(max(df['emb'].to_list()[:(b + 1) * batch_size_test][-31:], key=len))\n",
    "            # print(len(df_train['emb'].to_list()[:b+batch_size][-7:]))\n",
    "            x_test = np.full((batch_size_test, timesteps, num_features), -99.)\n",
    "            y_test = np.zeros((batch_size_test,  1))\n",
    "            for i in range(batch_size_test):\n",
    "                li = b * batch_size_test + i\n",
    "                x_test[i, 0:len(x_list[li]), :] = x_list[li]\n",
    "                y_test[i] = y_list[li]\n",
    "            yield x_test, y_test            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "642aa512",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_generator(df_train, batch[0][0], batch[0][1])\n",
    "val_data = val_generator(df_val, batch[1][0], batch[1][1])\n",
    "test_data = test_generator(df_test, batch[2][0], batch[2][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "af86ee49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def cul_all_metrics(y_true, y_pred, pos_label=1):\n",
    "    return {\"accuracy\": float(\"%.5f\" % accuracy_score(y_true=y_true, y_pred=y_pred)),\n",
    "            \"precision\": float(\"%.5f\" % precision_score(y_true=y_true, y_pred=y_pred, pos_label=pos_label, average=\"weighted\")),\n",
    "            \"recall\": float(\"%.5f\" % recall_score(y_true=y_true, y_pred=y_pred, pos_label=pos_label, average=\"weighted\")),\n",
    "            \"f1-score\": float(\"%.5f\" % f1_score(y_true=y_true, y_pred=y_pred, average=\"weighted\")),\n",
    "           }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081b520a",
   "metadata": {},
   "source": [
    "#### Step 5: Build \"Recurrence over VGCN_BERT\" Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "716ee802",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-06 11:35:18.482729: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2022-04-06 11:35:18.517168: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2099965000 Hz\n",
      "2022-04-06 11:35:18.521123: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5571cc7279d0 executing computations on platform Host. Devices:\n",
      "2022-04-06 11:35:18.521180: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "features (InputLayer)        [(None, None, 768)]       0         \n",
      "_________________________________________________________________\n",
      "masking (Masking)            (None, None, 768)         0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 100)               347600    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 30)                3030      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 62        \n",
      "=================================================================\n",
      "Total params: 350,692\n",
      "Trainable params: 350,692\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import h5py\n",
    "\n",
    "text_input = keras.Input(shape=(None,config_output_num_features,), dtype='float32', name='features')\n",
    "l_mask = keras.layers.Masking(mask_value=-99.)(text_input) \n",
    "encoded_text = keras.layers.LSTM(100,)(l_mask)\n",
    "out_dense = keras.layers.Dense(30, activation='relu')(encoded_text)\n",
    "out = keras.layers.Dense(2, activation='softmax')(out_dense)\n",
    "R_Model = keras.Model(text_input, out)\n",
    "R_Model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
    "R_Model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4987216e",
   "metadata": {},
   "outputs": [],
   "source": [
    "call_reduce = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_acc', factor=0.95, patience=3, verbose=2,\n",
    "                                mode='auto', min_delta=0.01, cooldown=0, min_lr=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "84358d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 227 steps, validate for 179 steps\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-06 11:35:25.517135: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_cudnn_lstm_with_fallback_5844_7301' and '__inference___backward_standard_lstm_7406_8003_specialized_for_StatefulPartitionedCall_at___inference_distributed_function_8125' both implement 'lstm_5a7fd80a-4385-4cb6-94ca-153bed2a49a2' but their signatures do not match.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/227 [============================>.] - ETA: 0s - loss: 0.6176 - acc: 0.6659"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-06 11:35:32.536233: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference_cudnn_lstm_with_fallback_9377' and '__inference_standard_lstm_9036_specialized_for_model_lstm_StatefulPartitionedCall_at___inference_distributed_function_10907' both implement 'lstm_2669f5cf-bbd9-440a-9f96-e0e2ea664434' but their signatures do not match.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "227/227 [==============================] - 13s 59ms/step - loss: 0.6161 - acc: 0.6667 - val_loss: 0.6116 - val_acc: 0.6750\n",
      "Epoch 2/10\n",
      "227/227 [==============================] - 6s 26ms/step - loss: 0.5295 - acc: 0.7311 - val_loss: 0.5692 - val_acc: 0.7337\n",
      "Epoch 3/10\n",
      "227/227 [==============================] - 6s 25ms/step - loss: 0.4880 - acc: 0.7581 - val_loss: 0.5373 - val_acc: 0.7533\n",
      "Epoch 4/10\n",
      "227/227 [==============================] - 6s 24ms/step - loss: 0.4683 - acc: 0.7702 - val_loss: 0.5016 - val_acc: 0.7756\n",
      "Epoch 5/10\n",
      "227/227 [==============================] - 5s 24ms/step - loss: 0.4536 - acc: 0.7799 - val_loss: 0.4825 - val_acc: 0.7812\n",
      "Epoch 6/10\n",
      "227/227 [==============================] - 5s 23ms/step - loss: 0.4412 - acc: 0.7878 - val_loss: 0.4720 - val_acc: 0.7821\n",
      "Epoch 7/10\n",
      "227/227 [==============================] - 5s 23ms/step - loss: 0.4302 - acc: 0.7930 - val_loss: 0.4618 - val_acc: 0.7886\n",
      "Epoch 8/10\n",
      "227/227 [==============================] - 5s 23ms/step - loss: 0.4250 - acc: 0.7972 - val_loss: 0.4688 - val_acc: 0.7905\n",
      "Epoch 9/10\n",
      "227/227 [==============================] - 5s 23ms/step - loss: 0.4158 - acc: 0.8010 - val_loss: 0.4469 - val_acc: 0.7924\n",
      "Epoch 10/10\n",
      "225/227 [============================>.] - ETA: 0s - loss: 0.4094 - acc: 0.8062\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0009500000451225787.\n",
      "227/227 [==============================] - 5s 23ms/step - loss: 0.4082 - acc: 0.8071 - val_loss: 0.4324 - val_acc: 0.7980\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f287e8a05d0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches_per_epoch = batch[0][1]\n",
    "\n",
    "batches_per_epoch_val= batch[1][1]\n",
    "\n",
    "R_Model.fit(train_data, steps_per_epoch=batches_per_epoch, epochs=10,\n",
    "                    validation_data=val_data, validation_steps=batches_per_epoch_val, callbacks =[call_reduce] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7edaee8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_per_epoch_test = batch[2][1]\n",
    "\n",
    "test_data = test_generator(df_test, batch[2][0], batch[2][1])\n",
    "r_score = R_Model.predict_generator(test_data, steps=batches_per_epoch_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fb268b59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.78864,\n",
       " 'precision': 0.78556,\n",
       " 'recall': 0.78864,\n",
       " 'f1-score': 0.78027}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_pred = np.argmax(r_score, axis=1).tolist()\n",
    "label = df_test.label.to_list()\n",
    "\n",
    "cul_all_metrics(label, r_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc669746",
   "metadata": {},
   "source": [
    "#### Step 6: Build \"Transformer over VGCN_BERT\" Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dbd7cfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads=8):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        assert (\n",
    "            embed_dim % num_heads == 0\n",
    "        ), \"embedding dimension not divisible by num heads\"\n",
    "        self.projection_dim = embed_dim // num_heads\n",
    "        self.wq = keras.layers.Dense(embed_dim)\n",
    "        self.wk = keras.layers.Dense(embed_dim)\n",
    "        self.wv = keras.layers.Dense(embed_dim)\n",
    "        self.combine_heads = keras.layers.Dense(embed_dim)\n",
    "\n",
    "    def attention(self, q, k, v):\n",
    "        score = tf.matmul(q, k, transpose_b=True)\n",
    "        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "        scaled_score = score / tf.math.sqrt(dk)\n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
    "        output = tf.matmul(weights, v)\n",
    "        return output, weights\n",
    "\n",
    "    def separate_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, x):\n",
    "        # x.shape = [batch_size, seq_len, embedding_dim]\n",
    "        batch_size = tf.shape(x)[0]\n",
    "        q = self.wq(x)  # (batch_size, seq_len, embed_dim)\n",
    "        k = self.wk(x)  # (batch_size, seq_len, embed_dim)\n",
    "        v = self.wv(x)  # (batch_size, seq_len, embed_dim)\n",
    "        q = self.separate_heads(\n",
    "            q, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        k = self.separate_heads(\n",
    "            k, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        v = self.separate_heads(\n",
    "            v, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        attention, weights = self.attention(q, k, v)\n",
    "        attention = tf.transpose(\n",
    "            attention, perm=[0, 2, 1, 3]\n",
    "        )  # (batch_size, seq_len, num_heads, projection_dim)\n",
    "        concat_attention = tf.reshape(\n",
    "            attention, (batch_size, -1, self.embed_dim)\n",
    "        )  # (batch_size, seq_len, embed_dim)\n",
    "        output = self.combine_heads(\n",
    "            concat_attention\n",
    "        )  # (batch_size, seq_len, embed_dim)\n",
    "        return output\n",
    "    \n",
    "class TransformerLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerLayer, self).__init__()\n",
    "\n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [\n",
    "                keras.layers.Dense(ff_dim, activation=\"relu\"),\n",
    "                keras.layers.Dense(embed_dim),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training):\n",
    "        attn_output = self.att(x)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e5dfb2da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "features (InputLayer)        [(None, None, 768)]       0         \n",
      "_________________________________________________________________\n",
      "masking_1 (Masking)          (None, None, 768)         0         \n",
      "_________________________________________________________________\n",
      "transformer_layer (Transform (None, None, 768)         2415392   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               347600    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 30)                3030      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 2)                 62        \n",
      "=================================================================\n",
      "Total params: 2,766,084\n",
      "Trainable params: 2,766,084\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "text_input = keras.Input(shape=(None,config_output_num_features,), dtype='float32', name='features')\n",
    "l_mask = keras.layers.Masking(mask_value=-99.)(text_input) \n",
    "transformer_encodings = TransformerLayer(embed_dim=config_output_num_features, num_heads=1, ff_dim=32)(l_mask)\n",
    "encoded_texts = keras.layers.LSTM(100,)(transformer_encodings)\n",
    "out_dense = keras.layers.Dense(30, activation='relu')(encoded_texts)\n",
    "out = keras.layers.Dense(2, activation='softmax')(out_dense)\n",
    "T_Model = keras.Model(text_input, out)\n",
    "T_Model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "T_Model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "21e1681b",
   "metadata": {},
   "outputs": [],
   "source": [
    "call_reduce = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_acc', factor=0.95, patience=3, verbose=2,\n",
    "                                mode='auto', min_delta=0.01, cooldown=0, min_lr=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "35496c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 227 steps, validate for 179 steps\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-06 11:36:32.872598: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_cudnn_lstm_with_fallback_94593_94775' and '__inference___backward_standard_lstm_94880_95363_specialized_for_StatefulPartitionedCall_at___inference_distributed_function_95941' both implement 'lstm_5cefef48-f6aa-4c43-9249-c90a2f054e2c' but their signatures do not match.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/227 [============================>.] - ETA: 0s - loss: 0.6495 - acc: 0.6553"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-06 11:36:43.506183: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference_standard_lstm_97086' and '__inference_standard_lstm_97086_specialized_for_model_1_lstm_1_StatefulPartitionedCall_at___inference_distributed_function_97452' both implement 'lstm_201ff56b-b679-4b86-8de7-c773c793c34c' but their signatures do not match.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "227/227 [==============================] - 17s 74ms/step - loss: 0.6487 - acc: 0.6562 - val_loss: 0.6384 - val_acc: 0.6592\n",
      "Epoch 2/10\n",
      "227/227 [==============================] - 11s 48ms/step - loss: 0.6323 - acc: 0.6572 - val_loss: 0.6245 - val_acc: 0.6723\n",
      "Epoch 3/10\n",
      "227/227 [==============================] - 11s 49ms/step - loss: 0.5717 - acc: 0.6978 - val_loss: 0.7432 - val_acc: 0.6750\n",
      "Epoch 4/10\n",
      "227/227 [==============================] - 11s 48ms/step - loss: 0.5357 - acc: 0.7357 - val_loss: 0.5514 - val_acc: 0.7412\n",
      "Epoch 5/10\n",
      "227/227 [==============================] - 11s 48ms/step - loss: 0.4879 - acc: 0.7606 - val_loss: 0.6206 - val_acc: 0.7048\n",
      "Epoch 6/10\n",
      "227/227 [==============================] - 11s 49ms/step - loss: 0.5077 - acc: 0.7577 - val_loss: 0.5148 - val_acc: 0.7533\n",
      "Epoch 7/10\n",
      "227/227 [==============================] - 11s 49ms/step - loss: 0.4744 - acc: 0.7705 - val_loss: 0.4867 - val_acc: 0.7728\n",
      "Epoch 8/10\n",
      "227/227 [==============================] - 11s 48ms/step - loss: 0.4665 - acc: 0.7723 - val_loss: 0.6377 - val_acc: 0.6750\n",
      "Epoch 9/10\n",
      "227/227 [==============================] - 11s 48ms/step - loss: 0.4577 - acc: 0.7774 - val_loss: 0.4834 - val_acc: 0.7877\n",
      "Epoch 10/10\n",
      "227/227 [==============================] - 11s 48ms/step - loss: 0.4505 - acc: 0.7916 - val_loss: 0.4519 - val_acc: 0.7654\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f2644642850>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches_per_epoch = batch[0][1]\n",
    "\n",
    "batches_per_epoch_val= batch[1][1]\n",
    "\n",
    "T_Model.fit(train_data, steps_per_epoch=batches_per_epoch, epochs=10,\n",
    "                    validation_data=val_data, validation_steps=batches_per_epoch_val, callbacks =[call_reduce] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ca5768ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_per_epoch_test = batch[2][1]\n",
    "\n",
    "test_data = test_generator(df_test, batch[2][0], batch[2][1])\n",
    "t_score = T_Model.predict_generator(test_data, steps=batches_per_epoch_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d6ef4707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.76164,\n",
       " 'precision': 0.76664,\n",
       " 'recall': 0.76164,\n",
       " 'f1-score': 0.74002}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_pred = np.argmax(t_score, axis=1).tolist()\n",
    "label = df_test.label.to_list()\n",
    "\n",
    "cul_all_metrics(label, t_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e026654",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "409.6px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
